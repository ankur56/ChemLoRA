{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "from gptchem.gpt_classifier import GPTClassifier\n",
    "from gptchem.tuner import Tuner\n",
    "from gptchem.formatter import RegressionFormatter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING = {\n",
    "    \"t5\": [\"q\", \"v\"],\n",
    "    \"mt5\": [\"q\", \"v\"],\n",
    "    \"bart\": [\"q_proj\", \"v_proj\"],\n",
    "    \"gpt2\": [\"c_attn\"],\n",
    "    \"bloom\": [\"query_key_value\"],\n",
    "    \"blip-2\": [\"q\", \"v\", \"q_proj\", \"v_proj\"],\n",
    "    \"opt\": [\"q_proj\", \"v_proj\"],\n",
    "    \"gptj\": [\"q_proj\", \"v_proj\"],\n",
    "    \"gpt_neox\": [\"query_key_value\"],\n",
    "    \"gpt_neo\": [\"q_proj\", \"v_proj\"],\n",
    "    \"bert\": [\"query\", \"value\"],\n",
    "    \"roberta\": [\"query\", \"value\"],\n",
    "    \"xlm-roberta\": [\"query\", \"value\"],\n",
    "    \"electra\": [\"query\", \"value\"],\n",
    "    \"deberta-v2\": [\"query_proj\", \"value_proj\"],\n",
    "    \"deberta\": [\"in_proj\"],\n",
    "    \"layoutlm\": [\"query\", \"value\"],\n",
    "    \"llama\": [\"q_proj\", \"v_proj\"],\n",
    "    \"chatglm\": [\"query_key_value\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import List\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, EarlyStoppingCallback\n",
    "import os\n",
    "print(torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the data\n",
    "def get_data(prop_to_get):\n",
    "    \n",
    "    prop = {\"b3lyp\": \"B3LYP atomization energy in kcal/mol\",\n",
    "            \"g4mp2\": \"G4MP2 atomization energy in kcal/mol\",\n",
    "            \"en_diff\": \"atomization energy difference in kcal/mol\",\n",
    "            \"bandgap\": \"bandgap in Hartrees\"}\n",
    "    \n",
    "    def pickle_to_df(filename):\n",
    "        with open(filename, \"rb\") as handle:\n",
    "            qm9_data = pickle.load(handle)\n",
    "\n",
    "        # Convert the keys and values of the dictionary into separate lists\n",
    "        smiles_list = list(qm9_data.keys())\n",
    "        property_list = list(qm9_data.values())\n",
    "\n",
    "        # Extract the B3LYP atomization energy as a separate list\n",
    "        b3lyp_at_list = [prop[0] for prop in property_list]\n",
    "        \n",
    "        # Extract the G4MP2 atomization energy as a separate list\n",
    "        g4mp2_at_list = [prop[1] for prop in property_list]\n",
    "        \n",
    "        # Extract the (G4MP2-B3LYP) atomization energy difference as a separate list\n",
    "        en_diff_list = [prop[2] for prop in property_list]\n",
    "        \n",
    "        # Extract the bandgap as a separate list\n",
    "        bandgap = [prop[3] for prop in property_list]\n",
    "\n",
    "        df = pd.DataFrame(list(zip(smiles_list, b3lyp_at_list, g4mp2_at_list,en_diff_list,bandgap)),\n",
    "                   columns =[\"SMILES\", prop[\"b3lyp\"], prop[\"g4mp2\"], prop[\"en_diff\"], prop[\"bandgap\"]])\n",
    "        return df\n",
    "    \n",
    "    #unpickle the data\n",
    "    train_df = pickle_to_df(\"qm9_key_smiles_1_train_data_without_validation.pickle\")\n",
    "    val_df = pickle_to_df(\"qm9_key_smiles_1_validation_data.pickle\")\n",
    "    test_df = pickle_to_df(\"qm9_key_smiles_1_holdout_data.pickle\")\n",
    "    \n",
    "    #format the data as text for the LLM\n",
    "    formatter = RegressionFormatter(representation_column='SMILES',\n",
    "        label_column=prop[prop_to_get],\n",
    "        property_name=prop[prop_to_get],\n",
    "        num_digits=4\n",
    "        )\n",
    "    \n",
    "    df_train = formatter.format_many(train_df).drop(columns=[\"label\",\"representation\"], axis=1)\n",
    "    df_val = formatter.format_many(val_df).drop(columns=[\"label\",\"representation\"], axis=1)\n",
    "    df_test = formatter.format_many(test_df).drop(columns=[\"label\",\"representation\"], axis=1)\n",
    "    \n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    #dataframes\n",
    "    df_train: pd.DataFrame,\n",
    "    df_val: pd.DataFrame,\n",
    "    # model/data params\n",
    "    base_model: str = \"gpt2\",  # the only required argument\n",
    "    prop: str = \"b3lyp\",\n",
    "    output_dir: str = \"outputs\",\n",
    "    # training hyperparams\n",
    "    batch_size: int = 256,\n",
    "    num_epochs: int = 10,\n",
    "    learning_rate: float = 3e-4,\n",
    "    cutoff_len: int = 256,\n",
    "    # lora hyperparams\n",
    "    lora_r: int = 8,\n",
    "    lora_alpha: int = 16,\n",
    "    lora_dropout: float = 0.05,\n",
    "    lora_target_modules: List[str] = [\"\"],\n",
    "    # llm hyperparams\n",
    "    train_on_inputs: bool = True,  # if False, masks out inputs in loss\n",
    "    group_by_length: bool = False,  # faster, but produces an odd training loss curve\n",
    "    resume_from_checkpoint: str = None,  # either training checkpoint or final adapter\n",
    "    prompt_template_name: str = \"chemgpt\",  # The prompt template to use, will default to alpaca.\n",
    "):\n",
    "    \n",
    "    lora_target_modules = TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING[base_model]\n",
    "    if int(os.environ.get(\"LOCAL_RANK\", 0)) == 0:\n",
    "        print(\n",
    "            f\"Training LoRA model with params:\\n\"\n",
    "            f\"base_model: {base_model}\\n\"\n",
    "            f\"prop: {prop}\\n\"\n",
    "            f\"output_dir: {output_dir}_{prop}_{base_model}\\n\"\n",
    "            f\"batch_size: {batch_size}\\n\"\n",
    "            f\"num_epochs: {num_epochs}\\n\"\n",
    "            f\"learning_rate: {learning_rate}\\n\"\n",
    "            f\"cutoff_len: {cutoff_len}\\n\"\n",
    "            f\"lora_r: {lora_r}\\n\"\n",
    "            f\"lora_alpha: {lora_alpha}\\n\"\n",
    "            f\"lora_dropout: {lora_dropout}\\n\"\n",
    "            f\"lora_target_modules: {lora_target_modules}\\n\"\n",
    "            f\"train_on_inputs: {train_on_inputs}\\n\"\n",
    "            f\"group_by_length: {group_by_length}\\n\"\n",
    "            f\"resume_from_checkpoint: {resume_from_checkpoint or False}\\n\"\n",
    "            f\"prompt template: {prompt_template_name}\\n\"\n",
    "        )\n",
    "    assert (\n",
    "        base_model\n",
    "    ), \"Please specify a --base_model, e.g. --base_model='gpt2'\"\n",
    "    #\n",
    "    #gradient_accumulation_steps = 1\n",
    "\n",
    "    #tells us how to split the model between GPUs \n",
    "    #device_map = \"sequential\"\n",
    "    #world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "    #ddp = world_size != 1\n",
    "    #ddp = False\n",
    "    #if ddp:\n",
    "    #    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
    "    #    gradient_accumulation_steps = gradient_accumulation_steps // world_size\n",
    "    \n",
    "    #set up the model and tokenizer    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    #get the model with the desired name automatically\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model, \n",
    "        load_in_8bit=False,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map='sequential',\n",
    "    )    \n",
    "    #tokenizer settings\n",
    "    def tokenize(prompt):\n",
    "        result = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=cutoff_len,\n",
    "            padding=True,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        return result\n",
    "    #tokenize the full prompt\n",
    "    def tokenize_prompt(data_point):\n",
    "        full_prompt = data_point[\"prompt\"]+data_point[\"completion\"]\n",
    "        tokenized_full_prompt = tokenize(full_prompt)\n",
    "        return tokenized_full_prompt\n",
    "\n",
    "    model = prepare_model_for_int8_training(model)\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=lora_target_modules,\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, config)\n",
    "\n",
    "    train_data = Dataset.from_pandas(df_train).shuffle().map(tokenize_prompt)\n",
    "    val_data = Dataset.from_pandas(df_val).shuffle().map(tokenize_prompt)\n",
    "    \n",
    "    if resume_from_checkpoint:\n",
    "        # Check the available weights and load them\n",
    "        checkpoint_name = os.path.join(\n",
    "            resume_from_checkpoint, \"pytorch_model.bin\"\n",
    "        )  # Full checkpoint\n",
    "        if not os.path.exists(checkpoint_name):\n",
    "            checkpoint_name = os.path.join(\n",
    "                resume_from_checkpoint, \"adapter_model.bin\"\n",
    "            )  # only LoRA model - LoRA config above has to fit\n",
    "            resume_from_checkpoint = (\n",
    "                False  # So the trainer won't try loading its state\n",
    "            )\n",
    "        # The two files above have a different name depending on how they were saved, but are actually the same.\n",
    "        if os.path.exists(checkpoint_name):\n",
    "            print(f\"Restarting from {checkpoint_name}\")\n",
    "            adapters_weights = torch.load(checkpoint_name)\n",
    "            model = set_peft_model_state_dict(model, adapters_weights)\n",
    "        else:\n",
    "            print(f\"Checkpoint {checkpoint_name} not found\")\n",
    "\n",
    "    model.print_trainable_parameters()  # Be more transparent about the % of trainable params.\n",
    "    print(train_data[0])\n",
    "    print(val_data[0])\n",
    "    \"\"\"\n",
    "    if not ddp and torch.cuda.device_count() > 1:\n",
    "        # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n",
    "        model.is_parallelizable = True\n",
    "        model.model_parallel = True\n",
    "    \"\"\"\n",
    "    trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        args=transformers.TrainingArguments(\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            warmup_steps=10,\n",
    "            num_train_epochs=num_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            fp16=True,\n",
    "            optim=\"adamw_torch\",\n",
    "            evaluation_strategy=\"steps\",\n",
    "            save_strategy=\"steps\",\n",
    "            output_dir=output_dir+\"_\"+prop+\"_\"+base_model,\n",
    "            save_total_limit=3,\n",
    "            metric_for_best_model = 'eval_loss',\n",
    "            load_best_model_at_end=True,\n",
    "            #ddp_find_unused_parameters=False if ddp else None,\n",
    "            group_by_length=group_by_length,\n",
    "        ),\n",
    "        data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "    )\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    old_state_dict = model.state_dict\n",
    "    print(old_state_dict)\n",
    "    print(model.state_dict)\n",
    "    model.state_dict = (\n",
    "        lambda self, *_, **__: get_peft_model_state_dict(\n",
    "            self, old_state_dict()\n",
    "        )\n",
    "    ).__get__(model, type(model))\n",
    "    \n",
    "    \n",
    "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "\n",
    "    model.save_pretrained(output_dir+\"_\"+prop+\"_\"+base_model)\n",
    "\n",
    "    print(\n",
    "        \"\\n If there's a warning about missing keys above, please disregard :)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import GenerationConfig\n",
    "#from utils.callbacks import Iteratorize, Stream\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\" \n",
    "    \n",
    "def generate(\n",
    "    df_test: pd.DataFrame,\n",
    "    load_8bit: bool = False,\n",
    "    base_model: str = \"gpt2\",\n",
    "    lora_weights: str = \"outputs\",\n",
    "    prop: str = \"b3lyp\",\n",
    "    prompt_template: str = \"\",\n",
    "    cutoff_len: int = 256,\n",
    "):\n",
    "    #set up the model and tokenizer    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "    # might not be optimal, just trying to run the code\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    def tokenize(prompt):\n",
    "        result = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=cutoff_len,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    model= AutoModelForCausalLM.from_pretrained(\n",
    "        base_model, \n",
    "        load_in_8bit=False,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map='auto',\n",
    "    )    \n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        lora_weights+\"_\"+prop+\"_\"+base_model,\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    if not load_8bit:\n",
    "        model.half()  # seems to fix bugs for some users.\n",
    "\n",
    "    model.eval()\n",
    "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    def evaluate(\n",
    "        prompt,\n",
    "        temperature=0,\n",
    "        top_p=0.75,\n",
    "        top_k=40,\n",
    "        num_beams=2,\n",
    "        max_new_tokens=128,\n",
    "        stream_output=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        inputs = tokenize(prompt)\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        generation_config = GenerationConfig(\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            num_beams=num_beams,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        generate_params = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"generation_config\": generation_config,\n",
    "            \"return_dict_in_generate\": True,\n",
    "            \"output_scores\": True,\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "        }\n",
    "\n",
    "        # Generate outs without streaming\n",
    "        with torch.no_grad():\n",
    "            generation_output = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                generation_config=generation_config,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "            )\n",
    "        s = generation_output.sequences[0]\n",
    "        output = tokenizer.decode(s)\n",
    "        #print(output)\n",
    "        return output\n",
    "\n",
    "    #convert to a number if we can, else none\n",
    "    def toFloat(x):\n",
    "        try:\n",
    "            return float(x)\n",
    "        except:\n",
    "            return None \n",
    "    \n",
    "    df_test[\"model_out\"] = df_test[\"prompt\"].map(lambda x: evaluate(x))\n",
    "    df_test[\"energy_out\"] = df_test[\"model_out\"].map(lambda x: toFloat(x.replace('###','@@@').split('@@@')[1]))\n",
    "    df_test[\"energy_true\"] = df_test[\"completion\"].map(lambda x: toFloat(x.split('@@@')[0]))\n",
    "    return(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(model):\n",
    "    df_train,df_val,df_test = get_data(\"b3lyp\")\n",
    "    train(df_train, df_val, base_model=model, prop=\"b3lyp\")\n",
    "    outs=generate(df_test.head(100), base_model=model, prop=\"b3lyp\")\n",
    "    outs.to_json(f\"outputs_{model}_b3lyp.json\")\n",
    "    plt.scatter(outs['energy_true'], outs['energy_out'])\n",
    "    pearsonr(outs['energy_true'], outs['energy_out'])\n",
    "    \n",
    "    df_train,df_val,df_test = get_data(\"g4mp2\")\n",
    "    train(df_train, df_val, base_model=model, prop=\"g4mp2\")\n",
    "    outs=generate(df_test.head(100), base_model=model, prop=\"g4mp2\")\n",
    "    outs.to_json(f\"outputs_{model}_g4mp2.json\")\n",
    "    plt.scatter(outs['energy_true'], outs['energy_out'])\n",
    "    pearsonr(outs['energy_true'], outs['energy_out'])\n",
    "    \n",
    "    df_train,df_val,df_test = get_data(\"en_diff\")\n",
    "    train(df_train, df_val, base_model=model, prop=\"en_diff\")\n",
    "    outs=generate(df_test.head(100), base_model=model, prop=\"en_diff\")\n",
    "    outs.to_json(f\"outputs_{model}_en_diff.json\")\n",
    "    plt.scatter(outs['energy_true'], outs['energy_out'])\n",
    "    pearsonr(outs['energy_true'], outs['energy_out'])\n",
    "\n",
    "    df_train,df_val,df_test = get_data(\"bandgap\")\n",
    "    train(df_train, df_val, base_model=model, prop=\"bandgap\")\n",
    "    outs=generate(df_test.head(100), base_model=model, prop=\"bandgap\")\n",
    "    outs.to_json(f\"outputs_{model}_bandgap.json\")\n",
    "    plt.scatter(outs['energy_true'], outs['energy_out'])\n",
    "    pearsonr(outs['energy_true'], outs['energy_out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run_all(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemGPT",
   "language": "python",
   "name": "chemgpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
