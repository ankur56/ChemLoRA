{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/g/m/gmerz2/miniconda3/envs/chemGPT/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "from gptchem.gpt_classifier import GPTClassifier\n",
    "from gptchem.tuner import Tuner\n",
    "from gptchem.formatter import RegressionFormatter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "import pickle\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING = {\n",
    "    \"t5\": [\"q\", \"v\"],\n",
    "    \"mt5\": [\"q\", \"v\"],\n",
    "    \"bart\": [\"q_proj\", \"v_proj\"],\n",
    "    \"gpt2\": [\"c_attn\"],\n",
    "    \"bloom\": [\"query_key_value\"],\n",
    "    \"blip-2\": [\"q\", \"v\", \"q_proj\", \"v_proj\"],\n",
    "    \"opt\": [\"q_proj\", \"v_proj\"],\n",
    "    \"gptj\": [\"q_proj\", \"v_proj\"],\n",
    "    \"gpt_neox\": [\"query_key_value\"],\n",
    "    \"gpt_neo\": [\"q_proj\", \"v_proj\"],\n",
    "    \"bert\": [\"query\", \"value\"],\n",
    "    \"roberta\": [\"query\", \"value\"],\n",
    "    \"xlm-roberta\": [\"query\", \"value\"],\n",
    "    \"electra\": [\"query\", \"value\"],\n",
    "    \"deberta-v2\": [\"query_proj\", \"value_proj\"],\n",
    "    \"deberta\": [\"in_proj\"],\n",
    "    \"layoutlm\": [\"query\", \"value\"],\n",
    "    \"llama\": [\"q_proj\", \"v_proj\"],\n",
    "    \"chatglm\": [\"query_key_value\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 21:48:52.469906: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-12 21:49:54.351941: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 121\n",
      "CUDA SETUP: Loading binary /u/g/m/gmerz2/miniconda3/envs/chemGPT/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/g/m/gmerz2/miniconda3/envs/chemGPT/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /u/g/m/gmerz2/miniconda3 did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/u/g/m/gmerz2/miniconda3/envs/chemGPT/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {Path('FILE')}\n",
      "  warn(msg)\n",
      "/u/g/m/gmerz2/miniconda3/envs/chemGPT/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {Path('/run/user/2289/docker.sock'), Path('unix')}\n",
      "  warn(msg)\n",
      "/u/g/m/gmerz2/miniconda3/envs/chemGPT/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {Path('//matplotlib_inline.backend_inline'), Path('module')}\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from typing import List\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, EarlyStoppingCallback\n",
    "import os\n",
    "print(torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the data\n",
    "def get_data(prop_to_get):\n",
    "    \n",
    "    prop = {\"b3lyp\": \"B3LYP atomization energy in kcal/mol\",\n",
    "            \"g4mp2\": \"G4MP2 atomization energy in kcal/mol\",\n",
    "            \"en_diff\": \"(G4MP2-B3LYP) atomization energy in kcal/mol\",\n",
    "            \"bandgap\": \"bandgap in Hartrees\"}\n",
    "    \n",
    "    def pickle_to_df(filename):\n",
    "        with open(\"qm9_key_smiles_1_validation_data.pickle\", \"rb\") as handle:\n",
    "            qm9_data = pickle.load(handle)\n",
    "\n",
    "        # Convert the keys and values of the dictionary into separate lists\n",
    "        smiles_list = list(qm9_data.keys())\n",
    "        property_list = list(qm9_data.values())\n",
    "\n",
    "        # Extract the B3LYP atomization energy as a separate list\n",
    "        b3lyp_at_list = [prop[0] for prop in property_list]\n",
    "        \n",
    "        # Extract the G4MP2 atomization energy as a separate list\n",
    "        g4mp2_at_list = [prop[1] for prop in property_list]\n",
    "        \n",
    "        # Extract the (G4MP2-B3LYP) atomization energy difference as a separate list\n",
    "        en_diff_list = [prop[2] for prop in property_list]\n",
    "        \n",
    "        # Extract the bandgap as a separate list\n",
    "        bandgap = [prop[3] for prop in property_list]\n",
    "\n",
    "        df = pd.DataFrame(list(zip(smiles_list, b3lyp_at_list, g4mp2_at_list,en_diff_list,bandgap)),\n",
    "                   columns =[\"SMILES\", prop[\"b3lyp\"], prop[\"g4mp2\"], prop[\"en_diff\"], prop[\"bandgap\"]])\n",
    "        return df\n",
    "    \n",
    "    #unpickle the data\n",
    "    train_df = pickle_to_df(\"qm9_key_smiles_1_train_data_without_validation.pickle\")\n",
    "    val_df = pickle_to_df(\"qm9_key_smiles_1_validation_data.pickle\")\n",
    "    test_df = pickle_to_df(\"qm9_key_smiles_1_holdout_data.pickle\")\n",
    "    \n",
    "    #format the data as text for the LLM\n",
    "    formatter = RegressionFormatter(representation_column='SMILES',\n",
    "        label_column=prop[prop_to_get],\n",
    "        property_name=prop[prop_to_get],\n",
    "        num_digits=4\n",
    "        )\n",
    "    \n",
    "    df_train = formatter.format_many(train_df).drop(columns=[\"label\",\"representation\"], axis=1)\n",
    "    df_val = formatter.format_many(val_df).drop(columns=[\"label\",\"representation\"], axis=1)\n",
    "    df_test = formatter.format_many(test_df).drop(columns=[\"label\",\"representation\"], axis=1)\n",
    "    \n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    #dataframes\n",
    "    df_train: pd.DataFrame,\n",
    "    df_val: pd.DataFrame,\n",
    "    # model/data params\n",
    "    base_model: str = \"gpt2\",  # the only required argument\n",
    "    prop: str = \"b3lyp\",\n",
    "    output_dir: str = \"outputs\",\n",
    "    # training hyperparams\n",
    "    batch_size: int = 1024,\n",
    "    micro_batch_size: int = 64,\n",
    "    num_epochs: int = 8,\n",
    "    learning_rate: float = 3e-4,\n",
    "    cutoff_len: int = 256,\n",
    "    # lora hyperparams\n",
    "    lora_r: int = 8,\n",
    "    lora_alpha: int = 16,\n",
    "    lora_dropout: float = 0.05,\n",
    "    lora_target_modules: List[str] = [\"\"],\n",
    "    # llm hyperparams\n",
    "    train_on_inputs: bool = True,  # if False, masks out inputs in loss\n",
    "    group_by_length: bool = False,  # faster, but produces an odd training loss curve\n",
    "    resume_from_checkpoint: str = None,  # either training checkpoint or final adapter\n",
    "    prompt_template_name: str = \"chemgpt\",  # The prompt template to use, will default to alpaca.\n",
    "):\n",
    "    \n",
    "    lora_target_modules = TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING[base_model]\n",
    "    if int(os.environ.get(\"LOCAL_RANK\", 0)) == 0:\n",
    "        print(\n",
    "            f\"Training LoRA model with params:\\n\"\n",
    "            f\"base_model: {base_model}\\n\"\n",
    "            f\"prop: {prop}\\n\"\n",
    "            f\"output_dir: {output_dir}_{prop}_{base_model}\\n\"\n",
    "            f\"batch_size: {batch_size}\\n\"\n",
    "            f\"micro_batch_size: {micro_batch_size}\\n\"\n",
    "            f\"num_epochs: {num_epochs}\\n\"\n",
    "            f\"learning_rate: {learning_rate}\\n\"\n",
    "            f\"cutoff_len: {cutoff_len}\\n\"\n",
    "            f\"lora_r: {lora_r}\\n\"\n",
    "            f\"lora_alpha: {lora_alpha}\\n\"\n",
    "            f\"lora_dropout: {lora_dropout}\\n\"\n",
    "            f\"lora_target_modules: {lora_target_modules}\\n\"\n",
    "            f\"train_on_inputs: {train_on_inputs}\\n\"\n",
    "            f\"group_by_length: {group_by_length}\\n\"\n",
    "            f\"resume_from_checkpoint: {resume_from_checkpoint or False}\\n\"\n",
    "            f\"prompt template: {prompt_template_name}\\n\"\n",
    "        )\n",
    "    assert (\n",
    "        base_model\n",
    "    ), \"Please specify a --base_model, e.g. --base_model='gpt2'\"\n",
    "    #\n",
    "    gradient_accumulation_steps = batch_size // micro_batch_size\n",
    "\n",
    "    #tells us how to split the model between GPUs \n",
    "    device_map = \"sequential\"\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "    #ddp = world_size != 1\n",
    "    ddp = False\n",
    "    if ddp:\n",
    "        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
    "        gradient_accumulation_steps = gradient_accumulation_steps // world_size\n",
    "    \n",
    "    #set up the model and tokenizer    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    #get the model with the desired name automatically\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model, \n",
    "        load_in_8bit=False,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map='sequential',\n",
    "    )    \n",
    "    #tokenizer settings\n",
    "    def tokenize(prompt):\n",
    "        result = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=cutoff_len,\n",
    "            padding=True,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        return result\n",
    "    #tokenize the full prompt\n",
    "    def tokenize_prompt(data_point):\n",
    "        full_prompt = data_point[\"prompt\"]+data_point[\"completion\"]\n",
    "        tokenized_full_prompt = tokenize(full_prompt)\n",
    "        return tokenized_full_prompt\n",
    "\n",
    "    model = prepare_model_for_int8_training(model)\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=lora_target_modules,\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, config)\n",
    "\n",
    "    train_data = Dataset.from_pandas(df_train).shuffle().map(tokenize_prompt)\n",
    "    val_data = Dataset.from_pandas(df_val).shuffle().map(tokenize_prompt)\n",
    "    test_data = Dataset.from_pandas(df_test).shuffle().map(tokenize_prompt)\n",
    "    \n",
    "    if resume_from_checkpoint:\n",
    "        # Check the available weights and load them\n",
    "        checkpoint_name = os.path.join(\n",
    "            resume_from_checkpoint, \"pytorch_model.bin\"\n",
    "        )  # Full checkpoint\n",
    "        if not os.path.exists(checkpoint_name):\n",
    "            checkpoint_name = os.path.join(\n",
    "                resume_from_checkpoint, \"adapter_model.bin\"\n",
    "            )  # only LoRA model - LoRA config above has to fit\n",
    "            resume_from_checkpoint = (\n",
    "                False  # So the trainer won't try loading its state\n",
    "            )\n",
    "        # The two files above have a different name depending on how they were saved, but are actually the same.\n",
    "        if os.path.exists(checkpoint_name):\n",
    "            print(f\"Restarting from {checkpoint_name}\")\n",
    "            adapters_weights = torch.load(checkpoint_name)\n",
    "            model = set_peft_model_state_dict(model, adapters_weights)\n",
    "        else:\n",
    "            print(f\"Checkpoint {checkpoint_name} not found\")\n",
    "\n",
    "    model.print_trainable_parameters()  # Be more transparent about the % of trainable params.\n",
    "    \n",
    "    if not ddp and torch.cuda.device_count() > 1:\n",
    "        # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n",
    "        model.is_parallelizable = True\n",
    "        model.model_parallel = True\n",
    "   \n",
    "    trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        args=transformers.TrainingArguments(\n",
    "            per_device_train_batch_size=micro_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            warmup_steps=10,\n",
    "            num_train_epochs=num_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            fp16=True,\n",
    "            logging_steps=4,\n",
    "            optim=\"adamw_torch\",\n",
    "            evaluation_strategy=\"steps\",\n",
    "            save_strategy=\"steps\",\n",
    "            eval_steps=4,\n",
    "            save_steps=4,\n",
    "            output_dir=output_dir+\"_\"+prop+\"_\"+base_model,\n",
    "            save_total_limit=3,\n",
    "            metric_for_best_model = 'eval_loss',\n",
    "            load_best_model_at_end=True,\n",
    "            ddp_find_unused_parameters=False if ddp else None,\n",
    "            group_by_length=group_by_length,\n",
    "        ),\n",
    "        data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "    )\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    old_state_dict = model.state_dict\n",
    "    print(old_state_dict)\n",
    "    print(model.state_dict)\n",
    "    model.state_dict = (\n",
    "        lambda self, *_, **__: get_peft_model_state_dict(\n",
    "            self, old_state_dict()\n",
    "        )\n",
    "    ).__get__(model, type(model))\n",
    "    \n",
    "    \n",
    "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "\n",
    "    model.save_pretrained(output_dir+\"_\"+prop+\"_\"+base_model)\n",
    "\n",
    "    print(\n",
    "        \"\\n If there's a warning about missing keys above, please disregard :)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import GenerationConfig\n",
    "#from utils.callbacks import Iteratorize, Stream\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\" \n",
    "    \n",
    "def generate(\n",
    "    df_test: pd.DataFrame,\n",
    "    load_8bit: bool = False,\n",
    "    base_model: str = \"gpt2\",\n",
    "    lora_weights: str = \"outputs\",\n",
    "    prop: str = \"b3lyp\",\n",
    "    prompt_template: str = \"\",\n",
    "    cutoff_len: int = 256,\n",
    "):\n",
    "    #set up the model and tokenizer    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "    # might not be optimal, just trying to run the code\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    def tokenize(prompt):\n",
    "        result = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=cutoff_len,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    model= AutoModelForCausalLM.from_pretrained(\n",
    "        base_model, \n",
    "        load_in_8bit=False,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map='sequential',\n",
    "    )    \n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        lora_weights+\"_\"+prop+\"_\"+base_model,\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    if not load_8bit:\n",
    "        model.half()  # seems to fix bugs for some users.\n",
    "\n",
    "    model.eval()\n",
    "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    def evaluate(\n",
    "        prompt,\n",
    "        temperature=0,\n",
    "        top_p=0.75,\n",
    "        top_k=40,\n",
    "        num_beams=2,\n",
    "        max_new_tokens=128,\n",
    "        stream_output=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        inputs = tokenize(prompt)\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        generation_config = GenerationConfig(\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            num_beams=num_beams,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        generate_params = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"generation_config\": generation_config,\n",
    "            \"return_dict_in_generate\": True,\n",
    "            \"output_scores\": True,\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "        }\n",
    "\n",
    "        # Generate outs without streaming\n",
    "        with torch.no_grad():\n",
    "            generation_output = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                generation_config=generation_config,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "            )\n",
    "        s = generation_output.sequences[0]\n",
    "        output = tokenizer.decode(s)\n",
    "        #print(output)\n",
    "        return output\n",
    "\n",
    "    #convert to a number if we can, else none\n",
    "    def toFloat(x):\n",
    "        try:\n",
    "            return float(x)\n",
    "        except:\n",
    "            return None \n",
    "    \n",
    "    df_test[\"model_out\"] = df_test[\"prompt\"].map(lambda x: evaluate(x))\n",
    "    df_test[\"energy_out\"] = df_test[\"model_out\"].map(lambda x: toFloat(x.replace('###','@@@').split('@@@')[1]))\n",
    "    df_test[\"energy_true\"] = df_test[\"completion\"].map(lambda x: toFloat(x.split('@@@')[0]))\n",
    "    return(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3219625/171233279.py:106: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"model_out\"] = df_test[\"prompt\"].map(lambda x: evaluate(x))\n",
      "/tmp/ipykernel_3219625/171233279.py:107: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"energy_out\"] = df_test[\"model_out\"].map(lambda x: toFloat(x.replace('###','@@@').split('@@@')[1]))\n",
      "/tmp/ipykernel_3219625/171233279.py:108: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"energy_true\"] = df_test[\"completion\"].map(lambda x: toFloat(x.split('@@@')[0]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LoRA model with params:\n",
      "base_model: gpt2\n",
      "prop: g4mp2\n",
      "output_dir: outputs_g4mp2_gpt2\n",
      "batch_size: 1024\n",
      "micro_batch_size: 64\n",
      "num_epochs: 8\n",
      "learning_rate: 0.0003\n",
      "cutoff_len: 256\n",
      "lora_r: 8\n",
      "lora_alpha: 16\n",
      "lora_dropout: 0.05\n",
      "lora_target_modules: ['c_attn']\n",
      "train_on_inputs: True\n",
      "group_by_length: False\n",
      "resume_from_checkpoint: False\n",
      "prompt template: chemgpt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294912 || all params: 124734720 || trainable%: 0.23643136409814364\n",
      "<bound method Module.state_dict of PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GPT2LMHeadModel(\n",
      "      (transformer): GPT2Model(\n",
      "        (wte): Embedding(50257, 768)\n",
      "        (wpe): Embedding(1024, 768)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (h): ModuleList(\n",
      "          (0-11): 12 x GPT2Block(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPT2Attention(\n",
      "              (c_attn): MergedLinear(\n",
      "                in_features=768, out_features=2304, bias=True\n",
      "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
      "                (lora_A): Linear(in_features=768, out_features=16, bias=False)\n",
      "                (lora_B): Conv1d(16, 1536, kernel_size=(1,), stride=(1,), groups=2, bias=False)\n",
      "              )\n",
      "              (c_proj): Conv1D()\n",
      "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPT2MLP(\n",
      "              (c_fc): Conv1D()\n",
      "              (c_proj): Conv1D()\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (lm_head): CastOutputToFloat(\n",
      "        (0): Linear(in_features=768, out_features=50257, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")>\n",
      "<bound method Module.state_dict of PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GPT2LMHeadModel(\n",
      "      (transformer): GPT2Model(\n",
      "        (wte): Embedding(50257, 768)\n",
      "        (wpe): Embedding(1024, 768)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (h): ModuleList(\n",
      "          (0-11): 12 x GPT2Block(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPT2Attention(\n",
      "              (c_attn): MergedLinear(\n",
      "                in_features=768, out_features=2304, bias=True\n",
      "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
      "                (lora_A): Linear(in_features=768, out_features=16, bias=False)\n",
      "                (lora_B): Conv1d(16, 1536, kernel_size=(1,), stride=(1,), groups=2, bias=False)\n",
      "              )\n",
      "              (c_proj): Conv1D()\n",
      "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPT2MLP(\n",
      "              (c_fc): Conv1D()\n",
      "              (c_proj): Conv1D()\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (lm_head): CastOutputToFloat(\n",
      "        (0): Linear(in_features=768, out_features=50257, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='88' max='88' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [88/88 11:52, Epoch 7/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.408200</td>\n",
       "      <td>5.242077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>5.393200</td>\n",
       "      <td>5.157229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>5.268300</td>\n",
       "      <td>4.954391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>5.064600</td>\n",
       "      <td>4.694221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.836700</td>\n",
       "      <td>4.442646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>4.571000</td>\n",
       "      <td>4.220270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>4.345700</td>\n",
       "      <td>4.013699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>4.120200</td>\n",
       "      <td>3.802003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>3.900100</td>\n",
       "      <td>3.583914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.702800</td>\n",
       "      <td>3.359756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>3.495300</td>\n",
       "      <td>3.142583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>3.323400</td>\n",
       "      <td>2.949406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>3.155800</td>\n",
       "      <td>2.782635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>3.016000</td>\n",
       "      <td>2.632330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.872300</td>\n",
       "      <td>2.500270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>2.777000</td>\n",
       "      <td>2.388752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>2.667500</td>\n",
       "      <td>2.295373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>2.597000</td>\n",
       "      <td>2.224075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>2.529400</td>\n",
       "      <td>2.166456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.474700</td>\n",
       "      <td>2.126215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>2.453300</td>\n",
       "      <td>2.101919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>2.426100</td>\n",
       "      <td>2.093653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['base_model.model.transformer.wte.weight', 'base_model.model.transformer.wpe.weight', 'base_model.model.transformer.h.0.ln_1.weight', 'base_model.model.transformer.h.0.ln_1.bias', 'base_model.model.transformer.h.0.attn.bias', 'base_model.model.transformer.h.0.attn.masked_bias', 'base_model.model.transformer.h.0.attn.c_attn.weight', 'base_model.model.transformer.h.0.attn.c_attn.bias', 'base_model.model.transformer.h.0.attn.c_proj.weight', 'base_model.model.transformer.h.0.attn.c_proj.bias', 'base_model.model.transformer.h.0.ln_2.weight', 'base_model.model.transformer.h.0.ln_2.bias', 'base_model.model.transformer.h.0.mlp.c_fc.weight', 'base_model.model.transformer.h.0.mlp.c_fc.bias', 'base_model.model.transformer.h.0.mlp.c_proj.weight', 'base_model.model.transformer.h.0.mlp.c_proj.bias', 'base_model.model.transformer.h.1.ln_1.weight', 'base_model.model.transformer.h.1.ln_1.bias', 'base_model.model.transformer.h.1.attn.bias', 'base_model.model.transformer.h.1.attn.masked_bias', 'base_model.model.transformer.h.1.attn.c_attn.weight', 'base_model.model.transformer.h.1.attn.c_attn.bias', 'base_model.model.transformer.h.1.attn.c_proj.weight', 'base_model.model.transformer.h.1.attn.c_proj.bias', 'base_model.model.transformer.h.1.ln_2.weight', 'base_model.model.transformer.h.1.ln_2.bias', 'base_model.model.transformer.h.1.mlp.c_fc.weight', 'base_model.model.transformer.h.1.mlp.c_fc.bias', 'base_model.model.transformer.h.1.mlp.c_proj.weight', 'base_model.model.transformer.h.1.mlp.c_proj.bias', 'base_model.model.transformer.h.2.ln_1.weight', 'base_model.model.transformer.h.2.ln_1.bias', 'base_model.model.transformer.h.2.attn.bias', 'base_model.model.transformer.h.2.attn.masked_bias', 'base_model.model.transformer.h.2.attn.c_attn.weight', 'base_model.model.transformer.h.2.attn.c_attn.bias', 'base_model.model.transformer.h.2.attn.c_proj.weight', 'base_model.model.transformer.h.2.attn.c_proj.bias', 'base_model.model.transformer.h.2.ln_2.weight', 'base_model.model.transformer.h.2.ln_2.bias', 'base_model.model.transformer.h.2.mlp.c_fc.weight', 'base_model.model.transformer.h.2.mlp.c_fc.bias', 'base_model.model.transformer.h.2.mlp.c_proj.weight', 'base_model.model.transformer.h.2.mlp.c_proj.bias', 'base_model.model.transformer.h.3.ln_1.weight', 'base_model.model.transformer.h.3.ln_1.bias', 'base_model.model.transformer.h.3.attn.bias', 'base_model.model.transformer.h.3.attn.masked_bias', 'base_model.model.transformer.h.3.attn.c_attn.weight', 'base_model.model.transformer.h.3.attn.c_attn.bias', 'base_model.model.transformer.h.3.attn.c_proj.weight', 'base_model.model.transformer.h.3.attn.c_proj.bias', 'base_model.model.transformer.h.3.ln_2.weight', 'base_model.model.transformer.h.3.ln_2.bias', 'base_model.model.transformer.h.3.mlp.c_fc.weight', 'base_model.model.transformer.h.3.mlp.c_fc.bias', 'base_model.model.transformer.h.3.mlp.c_proj.weight', 'base_model.model.transformer.h.3.mlp.c_proj.bias', 'base_model.model.transformer.h.4.ln_1.weight', 'base_model.model.transformer.h.4.ln_1.bias', 'base_model.model.transformer.h.4.attn.bias', 'base_model.model.transformer.h.4.attn.masked_bias', 'base_model.model.transformer.h.4.attn.c_attn.weight', 'base_model.model.transformer.h.4.attn.c_attn.bias', 'base_model.model.transformer.h.4.attn.c_proj.weight', 'base_model.model.transformer.h.4.attn.c_proj.bias', 'base_model.model.transformer.h.4.ln_2.weight', 'base_model.model.transformer.h.4.ln_2.bias', 'base_model.model.transformer.h.4.mlp.c_fc.weight', 'base_model.model.transformer.h.4.mlp.c_fc.bias', 'base_model.model.transformer.h.4.mlp.c_proj.weight', 'base_model.model.transformer.h.4.mlp.c_proj.bias', 'base_model.model.transformer.h.5.ln_1.weight', 'base_model.model.transformer.h.5.ln_1.bias', 'base_model.model.transformer.h.5.attn.bias', 'base_model.model.transformer.h.5.attn.masked_bias', 'base_model.model.transformer.h.5.attn.c_attn.weight', 'base_model.model.transformer.h.5.attn.c_attn.bias', 'base_model.model.transformer.h.5.attn.c_proj.weight', 'base_model.model.transformer.h.5.attn.c_proj.bias', 'base_model.model.transformer.h.5.ln_2.weight', 'base_model.model.transformer.h.5.ln_2.bias', 'base_model.model.transformer.h.5.mlp.c_fc.weight', 'base_model.model.transformer.h.5.mlp.c_fc.bias', 'base_model.model.transformer.h.5.mlp.c_proj.weight', 'base_model.model.transformer.h.5.mlp.c_proj.bias', 'base_model.model.transformer.h.6.ln_1.weight', 'base_model.model.transformer.h.6.ln_1.bias', 'base_model.model.transformer.h.6.attn.bias', 'base_model.model.transformer.h.6.attn.masked_bias', 'base_model.model.transformer.h.6.attn.c_attn.weight', 'base_model.model.transformer.h.6.attn.c_attn.bias', 'base_model.model.transformer.h.6.attn.c_proj.weight', 'base_model.model.transformer.h.6.attn.c_proj.bias', 'base_model.model.transformer.h.6.ln_2.weight', 'base_model.model.transformer.h.6.ln_2.bias', 'base_model.model.transformer.h.6.mlp.c_fc.weight', 'base_model.model.transformer.h.6.mlp.c_fc.bias', 'base_model.model.transformer.h.6.mlp.c_proj.weight', 'base_model.model.transformer.h.6.mlp.c_proj.bias', 'base_model.model.transformer.h.7.ln_1.weight', 'base_model.model.transformer.h.7.ln_1.bias', 'base_model.model.transformer.h.7.attn.bias', 'base_model.model.transformer.h.7.attn.masked_bias', 'base_model.model.transformer.h.7.attn.c_attn.weight', 'base_model.model.transformer.h.7.attn.c_attn.bias', 'base_model.model.transformer.h.7.attn.c_proj.weight', 'base_model.model.transformer.h.7.attn.c_proj.bias', 'base_model.model.transformer.h.7.ln_2.weight', 'base_model.model.transformer.h.7.ln_2.bias', 'base_model.model.transformer.h.7.mlp.c_fc.weight', 'base_model.model.transformer.h.7.mlp.c_fc.bias', 'base_model.model.transformer.h.7.mlp.c_proj.weight', 'base_model.model.transformer.h.7.mlp.c_proj.bias', 'base_model.model.transformer.h.8.ln_1.weight', 'base_model.model.transformer.h.8.ln_1.bias', 'base_model.model.transformer.h.8.attn.bias', 'base_model.model.transformer.h.8.attn.masked_bias', 'base_model.model.transformer.h.8.attn.c_attn.weight', 'base_model.model.transformer.h.8.attn.c_attn.bias', 'base_model.model.transformer.h.8.attn.c_proj.weight', 'base_model.model.transformer.h.8.attn.c_proj.bias', 'base_model.model.transformer.h.8.ln_2.weight', 'base_model.model.transformer.h.8.ln_2.bias', 'base_model.model.transformer.h.8.mlp.c_fc.weight', 'base_model.model.transformer.h.8.mlp.c_fc.bias', 'base_model.model.transformer.h.8.mlp.c_proj.weight', 'base_model.model.transformer.h.8.mlp.c_proj.bias', 'base_model.model.transformer.h.9.ln_1.weight', 'base_model.model.transformer.h.9.ln_1.bias', 'base_model.model.transformer.h.9.attn.bias', 'base_model.model.transformer.h.9.attn.masked_bias', 'base_model.model.transformer.h.9.attn.c_attn.weight', 'base_model.model.transformer.h.9.attn.c_attn.bias', 'base_model.model.transformer.h.9.attn.c_proj.weight', 'base_model.model.transformer.h.9.attn.c_proj.bias', 'base_model.model.transformer.h.9.ln_2.weight', 'base_model.model.transformer.h.9.ln_2.bias', 'base_model.model.transformer.h.9.mlp.c_fc.weight', 'base_model.model.transformer.h.9.mlp.c_fc.bias', 'base_model.model.transformer.h.9.mlp.c_proj.weight', 'base_model.model.transformer.h.9.mlp.c_proj.bias', 'base_model.model.transformer.h.10.ln_1.weight', 'base_model.model.transformer.h.10.ln_1.bias', 'base_model.model.transformer.h.10.attn.bias', 'base_model.model.transformer.h.10.attn.masked_bias', 'base_model.model.transformer.h.10.attn.c_attn.weight', 'base_model.model.transformer.h.10.attn.c_attn.bias', 'base_model.model.transformer.h.10.attn.c_proj.weight', 'base_model.model.transformer.h.10.attn.c_proj.bias', 'base_model.model.transformer.h.10.ln_2.weight', 'base_model.model.transformer.h.10.ln_2.bias', 'base_model.model.transformer.h.10.mlp.c_fc.weight', 'base_model.model.transformer.h.10.mlp.c_fc.bias', 'base_model.model.transformer.h.10.mlp.c_proj.weight', 'base_model.model.transformer.h.10.mlp.c_proj.bias', 'base_model.model.transformer.h.11.ln_1.weight', 'base_model.model.transformer.h.11.ln_1.bias', 'base_model.model.transformer.h.11.attn.bias', 'base_model.model.transformer.h.11.attn.masked_bias', 'base_model.model.transformer.h.11.attn.c_attn.weight', 'base_model.model.transformer.h.11.attn.c_attn.bias', 'base_model.model.transformer.h.11.attn.c_proj.weight', 'base_model.model.transformer.h.11.attn.c_proj.bias', 'base_model.model.transformer.h.11.ln_2.weight', 'base_model.model.transformer.h.11.ln_2.bias', 'base_model.model.transformer.h.11.mlp.c_fc.weight', 'base_model.model.transformer.h.11.mlp.c_fc.bias', 'base_model.model.transformer.h.11.mlp.c_proj.weight', 'base_model.model.transformer.h.11.mlp.c_proj.bias', 'base_model.model.transformer.ln_f.weight', 'base_model.model.transformer.ln_f.bias', 'base_model.model.lm_head.0.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " If there's a warning about missing keys above, please disregard :)\n",
      "11720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3219625/171233279.py:106: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"model_out\"] = df_test[\"prompt\"].map(lambda x: evaluate(x))\n",
      "/tmp/ipykernel_3219625/171233279.py:107: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"energy_out\"] = df_test[\"model_out\"].map(lambda x: toFloat(x.replace('###','@@@').split('@@@')[1]))\n",
      "/tmp/ipykernel_3219625/171233279.py:108: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"energy_true\"] = df_test[\"completion\"].map(lambda x: toFloat(x.split('@@@')[0]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LoRA model with params:\n",
      "base_model: gpt2\n",
      "prop: en_diff\n",
      "output_dir: outputs_en_diff_gpt2\n",
      "batch_size: 1024\n",
      "micro_batch_size: 64\n",
      "num_epochs: 8\n",
      "learning_rate: 0.0003\n",
      "cutoff_len: 256\n",
      "lora_r: 8\n",
      "lora_alpha: 16\n",
      "lora_dropout: 0.05\n",
      "lora_target_modules: ['c_attn']\n",
      "train_on_inputs: True\n",
      "group_by_length: False\n",
      "resume_from_checkpoint: False\n",
      "prompt template: chemgpt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294912 || all params: 124734720 || trainable%: 0.23643136409814364\n",
      "<bound method Module.state_dict of PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GPT2LMHeadModel(\n",
      "      (transformer): GPT2Model(\n",
      "        (wte): Embedding(50257, 768)\n",
      "        (wpe): Embedding(1024, 768)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (h): ModuleList(\n",
      "          (0-11): 12 x GPT2Block(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPT2Attention(\n",
      "              (c_attn): MergedLinear(\n",
      "                in_features=768, out_features=2304, bias=True\n",
      "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
      "                (lora_A): Linear(in_features=768, out_features=16, bias=False)\n",
      "                (lora_B): Conv1d(16, 1536, kernel_size=(1,), stride=(1,), groups=2, bias=False)\n",
      "              )\n",
      "              (c_proj): Conv1D()\n",
      "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPT2MLP(\n",
      "              (c_fc): Conv1D()\n",
      "              (c_proj): Conv1D()\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (lm_head): CastOutputToFloat(\n",
      "        (0): Linear(in_features=768, out_features=50257, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")>\n",
      "<bound method Module.state_dict of PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GPT2LMHeadModel(\n",
      "      (transformer): GPT2Model(\n",
      "        (wte): Embedding(50257, 768)\n",
      "        (wpe): Embedding(1024, 768)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (h): ModuleList(\n",
      "          (0-11): 12 x GPT2Block(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPT2Attention(\n",
      "              (c_attn): MergedLinear(\n",
      "                in_features=768, out_features=2304, bias=True\n",
      "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
      "                (lora_A): Linear(in_features=768, out_features=16, bias=False)\n",
      "                (lora_B): Conv1d(16, 1536, kernel_size=(1,), stride=(1,), groups=2, bias=False)\n",
      "              )\n",
      "              (c_proj): Conv1D()\n",
      "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPT2MLP(\n",
      "              (c_fc): Conv1D()\n",
      "              (c_proj): Conv1D()\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (lm_head): CastOutputToFloat(\n",
      "        (0): Linear(in_features=768, out_features=50257, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='88' max='88' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [88/88 12:01, Epoch 7/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.297600</td>\n",
       "      <td>5.164351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>5.258000</td>\n",
       "      <td>5.081279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>5.158800</td>\n",
       "      <td>4.890702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>4.964900</td>\n",
       "      <td>4.635651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.721900</td>\n",
       "      <td>4.388692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>4.492500</td>\n",
       "      <td>4.176734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>4.273200</td>\n",
       "      <td>3.972516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>4.067600</td>\n",
       "      <td>3.772846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>3.866500</td>\n",
       "      <td>3.571645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.680300</td>\n",
       "      <td>3.378829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>3.494700</td>\n",
       "      <td>3.186343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>3.326900</td>\n",
       "      <td>2.998811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>3.152900</td>\n",
       "      <td>2.819234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>2.990300</td>\n",
       "      <td>2.647573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.860000</td>\n",
       "      <td>2.499649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>2.737000</td>\n",
       "      <td>2.383332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>2.626300</td>\n",
       "      <td>2.296425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>2.556200</td>\n",
       "      <td>2.227604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>2.491800</td>\n",
       "      <td>2.172411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.441500</td>\n",
       "      <td>2.132216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>2.419100</td>\n",
       "      <td>2.108664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>2.398300</td>\n",
       "      <td>2.098531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['base_model.model.transformer.wte.weight', 'base_model.model.transformer.wpe.weight', 'base_model.model.transformer.h.0.ln_1.weight', 'base_model.model.transformer.h.0.ln_1.bias', 'base_model.model.transformer.h.0.attn.bias', 'base_model.model.transformer.h.0.attn.masked_bias', 'base_model.model.transformer.h.0.attn.c_attn.weight', 'base_model.model.transformer.h.0.attn.c_attn.bias', 'base_model.model.transformer.h.0.attn.c_proj.weight', 'base_model.model.transformer.h.0.attn.c_proj.bias', 'base_model.model.transformer.h.0.ln_2.weight', 'base_model.model.transformer.h.0.ln_2.bias', 'base_model.model.transformer.h.0.mlp.c_fc.weight', 'base_model.model.transformer.h.0.mlp.c_fc.bias', 'base_model.model.transformer.h.0.mlp.c_proj.weight', 'base_model.model.transformer.h.0.mlp.c_proj.bias', 'base_model.model.transformer.h.1.ln_1.weight', 'base_model.model.transformer.h.1.ln_1.bias', 'base_model.model.transformer.h.1.attn.bias', 'base_model.model.transformer.h.1.attn.masked_bias', 'base_model.model.transformer.h.1.attn.c_attn.weight', 'base_model.model.transformer.h.1.attn.c_attn.bias', 'base_model.model.transformer.h.1.attn.c_proj.weight', 'base_model.model.transformer.h.1.attn.c_proj.bias', 'base_model.model.transformer.h.1.ln_2.weight', 'base_model.model.transformer.h.1.ln_2.bias', 'base_model.model.transformer.h.1.mlp.c_fc.weight', 'base_model.model.transformer.h.1.mlp.c_fc.bias', 'base_model.model.transformer.h.1.mlp.c_proj.weight', 'base_model.model.transformer.h.1.mlp.c_proj.bias', 'base_model.model.transformer.h.2.ln_1.weight', 'base_model.model.transformer.h.2.ln_1.bias', 'base_model.model.transformer.h.2.attn.bias', 'base_model.model.transformer.h.2.attn.masked_bias', 'base_model.model.transformer.h.2.attn.c_attn.weight', 'base_model.model.transformer.h.2.attn.c_attn.bias', 'base_model.model.transformer.h.2.attn.c_proj.weight', 'base_model.model.transformer.h.2.attn.c_proj.bias', 'base_model.model.transformer.h.2.ln_2.weight', 'base_model.model.transformer.h.2.ln_2.bias', 'base_model.model.transformer.h.2.mlp.c_fc.weight', 'base_model.model.transformer.h.2.mlp.c_fc.bias', 'base_model.model.transformer.h.2.mlp.c_proj.weight', 'base_model.model.transformer.h.2.mlp.c_proj.bias', 'base_model.model.transformer.h.3.ln_1.weight', 'base_model.model.transformer.h.3.ln_1.bias', 'base_model.model.transformer.h.3.attn.bias', 'base_model.model.transformer.h.3.attn.masked_bias', 'base_model.model.transformer.h.3.attn.c_attn.weight', 'base_model.model.transformer.h.3.attn.c_attn.bias', 'base_model.model.transformer.h.3.attn.c_proj.weight', 'base_model.model.transformer.h.3.attn.c_proj.bias', 'base_model.model.transformer.h.3.ln_2.weight', 'base_model.model.transformer.h.3.ln_2.bias', 'base_model.model.transformer.h.3.mlp.c_fc.weight', 'base_model.model.transformer.h.3.mlp.c_fc.bias', 'base_model.model.transformer.h.3.mlp.c_proj.weight', 'base_model.model.transformer.h.3.mlp.c_proj.bias', 'base_model.model.transformer.h.4.ln_1.weight', 'base_model.model.transformer.h.4.ln_1.bias', 'base_model.model.transformer.h.4.attn.bias', 'base_model.model.transformer.h.4.attn.masked_bias', 'base_model.model.transformer.h.4.attn.c_attn.weight', 'base_model.model.transformer.h.4.attn.c_attn.bias', 'base_model.model.transformer.h.4.attn.c_proj.weight', 'base_model.model.transformer.h.4.attn.c_proj.bias', 'base_model.model.transformer.h.4.ln_2.weight', 'base_model.model.transformer.h.4.ln_2.bias', 'base_model.model.transformer.h.4.mlp.c_fc.weight', 'base_model.model.transformer.h.4.mlp.c_fc.bias', 'base_model.model.transformer.h.4.mlp.c_proj.weight', 'base_model.model.transformer.h.4.mlp.c_proj.bias', 'base_model.model.transformer.h.5.ln_1.weight', 'base_model.model.transformer.h.5.ln_1.bias', 'base_model.model.transformer.h.5.attn.bias', 'base_model.model.transformer.h.5.attn.masked_bias', 'base_model.model.transformer.h.5.attn.c_attn.weight', 'base_model.model.transformer.h.5.attn.c_attn.bias', 'base_model.model.transformer.h.5.attn.c_proj.weight', 'base_model.model.transformer.h.5.attn.c_proj.bias', 'base_model.model.transformer.h.5.ln_2.weight', 'base_model.model.transformer.h.5.ln_2.bias', 'base_model.model.transformer.h.5.mlp.c_fc.weight', 'base_model.model.transformer.h.5.mlp.c_fc.bias', 'base_model.model.transformer.h.5.mlp.c_proj.weight', 'base_model.model.transformer.h.5.mlp.c_proj.bias', 'base_model.model.transformer.h.6.ln_1.weight', 'base_model.model.transformer.h.6.ln_1.bias', 'base_model.model.transformer.h.6.attn.bias', 'base_model.model.transformer.h.6.attn.masked_bias', 'base_model.model.transformer.h.6.attn.c_attn.weight', 'base_model.model.transformer.h.6.attn.c_attn.bias', 'base_model.model.transformer.h.6.attn.c_proj.weight', 'base_model.model.transformer.h.6.attn.c_proj.bias', 'base_model.model.transformer.h.6.ln_2.weight', 'base_model.model.transformer.h.6.ln_2.bias', 'base_model.model.transformer.h.6.mlp.c_fc.weight', 'base_model.model.transformer.h.6.mlp.c_fc.bias', 'base_model.model.transformer.h.6.mlp.c_proj.weight', 'base_model.model.transformer.h.6.mlp.c_proj.bias', 'base_model.model.transformer.h.7.ln_1.weight', 'base_model.model.transformer.h.7.ln_1.bias', 'base_model.model.transformer.h.7.attn.bias', 'base_model.model.transformer.h.7.attn.masked_bias', 'base_model.model.transformer.h.7.attn.c_attn.weight', 'base_model.model.transformer.h.7.attn.c_attn.bias', 'base_model.model.transformer.h.7.attn.c_proj.weight', 'base_model.model.transformer.h.7.attn.c_proj.bias', 'base_model.model.transformer.h.7.ln_2.weight', 'base_model.model.transformer.h.7.ln_2.bias', 'base_model.model.transformer.h.7.mlp.c_fc.weight', 'base_model.model.transformer.h.7.mlp.c_fc.bias', 'base_model.model.transformer.h.7.mlp.c_proj.weight', 'base_model.model.transformer.h.7.mlp.c_proj.bias', 'base_model.model.transformer.h.8.ln_1.weight', 'base_model.model.transformer.h.8.ln_1.bias', 'base_model.model.transformer.h.8.attn.bias', 'base_model.model.transformer.h.8.attn.masked_bias', 'base_model.model.transformer.h.8.attn.c_attn.weight', 'base_model.model.transformer.h.8.attn.c_attn.bias', 'base_model.model.transformer.h.8.attn.c_proj.weight', 'base_model.model.transformer.h.8.attn.c_proj.bias', 'base_model.model.transformer.h.8.ln_2.weight', 'base_model.model.transformer.h.8.ln_2.bias', 'base_model.model.transformer.h.8.mlp.c_fc.weight', 'base_model.model.transformer.h.8.mlp.c_fc.bias', 'base_model.model.transformer.h.8.mlp.c_proj.weight', 'base_model.model.transformer.h.8.mlp.c_proj.bias', 'base_model.model.transformer.h.9.ln_1.weight', 'base_model.model.transformer.h.9.ln_1.bias', 'base_model.model.transformer.h.9.attn.bias', 'base_model.model.transformer.h.9.attn.masked_bias', 'base_model.model.transformer.h.9.attn.c_attn.weight', 'base_model.model.transformer.h.9.attn.c_attn.bias', 'base_model.model.transformer.h.9.attn.c_proj.weight', 'base_model.model.transformer.h.9.attn.c_proj.bias', 'base_model.model.transformer.h.9.ln_2.weight', 'base_model.model.transformer.h.9.ln_2.bias', 'base_model.model.transformer.h.9.mlp.c_fc.weight', 'base_model.model.transformer.h.9.mlp.c_fc.bias', 'base_model.model.transformer.h.9.mlp.c_proj.weight', 'base_model.model.transformer.h.9.mlp.c_proj.bias', 'base_model.model.transformer.h.10.ln_1.weight', 'base_model.model.transformer.h.10.ln_1.bias', 'base_model.model.transformer.h.10.attn.bias', 'base_model.model.transformer.h.10.attn.masked_bias', 'base_model.model.transformer.h.10.attn.c_attn.weight', 'base_model.model.transformer.h.10.attn.c_attn.bias', 'base_model.model.transformer.h.10.attn.c_proj.weight', 'base_model.model.transformer.h.10.attn.c_proj.bias', 'base_model.model.transformer.h.10.ln_2.weight', 'base_model.model.transformer.h.10.ln_2.bias', 'base_model.model.transformer.h.10.mlp.c_fc.weight', 'base_model.model.transformer.h.10.mlp.c_fc.bias', 'base_model.model.transformer.h.10.mlp.c_proj.weight', 'base_model.model.transformer.h.10.mlp.c_proj.bias', 'base_model.model.transformer.h.11.ln_1.weight', 'base_model.model.transformer.h.11.ln_1.bias', 'base_model.model.transformer.h.11.attn.bias', 'base_model.model.transformer.h.11.attn.masked_bias', 'base_model.model.transformer.h.11.attn.c_attn.weight', 'base_model.model.transformer.h.11.attn.c_attn.bias', 'base_model.model.transformer.h.11.attn.c_proj.weight', 'base_model.model.transformer.h.11.attn.c_proj.bias', 'base_model.model.transformer.h.11.ln_2.weight', 'base_model.model.transformer.h.11.ln_2.bias', 'base_model.model.transformer.h.11.mlp.c_fc.weight', 'base_model.model.transformer.h.11.mlp.c_fc.bias', 'base_model.model.transformer.h.11.mlp.c_proj.weight', 'base_model.model.transformer.h.11.mlp.c_proj.bias', 'base_model.model.transformer.ln_f.weight', 'base_model.model.transformer.ln_f.bias', 'base_model.model.lm_head.0.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " If there's a warning about missing keys above, please disregard :)\n",
      "11720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3219625/171233279.py:106: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"model_out\"] = df_test[\"prompt\"].map(lambda x: evaluate(x))\n",
      "/tmp/ipykernel_3219625/171233279.py:107: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"energy_out\"] = df_test[\"model_out\"].map(lambda x: toFloat(x.replace('###','@@@').split('@@@')[1]))\n",
      "/tmp/ipykernel_3219625/171233279.py:108: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"energy_true\"] = df_test[\"completion\"].map(lambda x: toFloat(x.split('@@@')[0]))\n"
     ]
    }
   ],
   "source": [
    "def run_all(model):\n",
    "    df_train,df_val, df_test = get_data(\"b3lyp\")\n",
    "    train(df_train, df_val, base_model=model, prop=\"b3lyp\")\n",
    "    outs=generate(df_test.head(1000), base_model=model, prop=\"b3lyp\")\n",
    "    outs.to_json(f\"outputs_{model}_b3lyp.json\")\n",
    "    \n",
    "    df_train,df_val, df_test = get_data(\"g4mp2\")\n",
    "    train(df_train, df_val, base_model=model, prop=\"g4mp2\")\n",
    "    outs=generate(df_test.head(1000), base_model=model, prop=\"g4mp2\")\n",
    "    outs.to_json(f\"outputs_{model}_g4mp2.json\")\n",
    "    \n",
    "    df_train,df_val, df_test = get_data(\"en_diff\")\n",
    "    train(df_train, df_val, base_model=model, prop=\"en_diff\")\n",
    "    outs=generate(df_test.head(1000), base_model=model, prop=\"en_diff\")\n",
    "    outs.to_json(f\"outputs_{model}_en_diff.json\")    \n",
    "\n",
    "    df_train,df_val, df_test = get_data(\"bandgap\")\n",
    "    train(df_train, df_val, base_model=model, prop=\"bandgap\")\n",
    "    outs=generate(df_test.head(1000), base_model=model, prop=\"bandgap\")\n",
    "    outs.to_json(f\"outputs_{model}_bandgap.json\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LoRA model with params:\n",
      "base_model: gpt2\n",
      "prop: bandgap\n",
      "output_dir: outputs_bandgap_gpt2\n",
      "batch_size: 1024\n",
      "micro_batch_size: 64\n",
      "num_epochs: 8\n",
      "learning_rate: 0.0003\n",
      "cutoff_len: 256\n",
      "lora_r: 8\n",
      "lora_alpha: 16\n",
      "lora_dropout: 0.05\n",
      "lora_target_modules: ['c_attn']\n",
      "train_on_inputs: True\n",
      "group_by_length: False\n",
      "resume_from_checkpoint: False\n",
      "prompt template: chemgpt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294912 || all params: 124734720 || trainable%: 0.23643136409814364\n",
      "<bound method Module.state_dict of PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GPT2LMHeadModel(\n",
      "      (transformer): GPT2Model(\n",
      "        (wte): Embedding(50257, 768)\n",
      "        (wpe): Embedding(1024, 768)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (h): ModuleList(\n",
      "          (0-11): 12 x GPT2Block(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPT2Attention(\n",
      "              (c_attn): MergedLinear(\n",
      "                in_features=768, out_features=2304, bias=True\n",
      "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
      "                (lora_A): Linear(in_features=768, out_features=16, bias=False)\n",
      "                (lora_B): Conv1d(16, 1536, kernel_size=(1,), stride=(1,), groups=2, bias=False)\n",
      "              )\n",
      "              (c_proj): Conv1D()\n",
      "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPT2MLP(\n",
      "              (c_fc): Conv1D()\n",
      "              (c_proj): Conv1D()\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (lm_head): CastOutputToFloat(\n",
      "        (0): Linear(in_features=768, out_features=50257, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")>\n",
      "<bound method Module.state_dict of PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GPT2LMHeadModel(\n",
      "      (transformer): GPT2Model(\n",
      "        (wte): Embedding(50257, 768)\n",
      "        (wpe): Embedding(1024, 768)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (h): ModuleList(\n",
      "          (0-11): 12 x GPT2Block(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPT2Attention(\n",
      "              (c_attn): MergedLinear(\n",
      "                in_features=768, out_features=2304, bias=True\n",
      "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
      "                (lora_A): Linear(in_features=768, out_features=16, bias=False)\n",
      "                (lora_B): Conv1d(16, 1536, kernel_size=(1,), stride=(1,), groups=2, bias=False)\n",
      "              )\n",
      "              (c_proj): Conv1D()\n",
      "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPT2MLP(\n",
      "              (c_fc): Conv1D()\n",
      "              (c_proj): Conv1D()\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (lm_head): CastOutputToFloat(\n",
      "        (0): Linear(in_features=768, out_features=50257, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='88' max='88' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [88/88 14:45, Epoch 7/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.864600</td>\n",
       "      <td>5.865883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>5.826500</td>\n",
       "      <td>5.787519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>5.728600</td>\n",
       "      <td>5.602403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>5.540500</td>\n",
       "      <td>5.347301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>5.282000</td>\n",
       "      <td>5.055130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>5.031300</td>\n",
       "      <td>4.775299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>4.772100</td>\n",
       "      <td>4.511669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>4.533200</td>\n",
       "      <td>4.253798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>4.292800</td>\n",
       "      <td>4.010244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.067400</td>\n",
       "      <td>3.774813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>3.854700</td>\n",
       "      <td>3.555833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>3.666500</td>\n",
       "      <td>3.353344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>3.487000</td>\n",
       "      <td>3.172784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>3.313500</td>\n",
       "      <td>3.010289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.187800</td>\n",
       "      <td>2.871279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>3.068800</td>\n",
       "      <td>2.745970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>2.946400</td>\n",
       "      <td>2.637721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>2.873300</td>\n",
       "      <td>2.550438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>2.787800</td>\n",
       "      <td>2.479304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.734700</td>\n",
       "      <td>2.423968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>2.700700</td>\n",
       "      <td>2.391870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>2.679300</td>\n",
       "      <td>2.378554</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['base_model.model.transformer.wte.weight', 'base_model.model.transformer.wpe.weight', 'base_model.model.transformer.h.0.ln_1.weight', 'base_model.model.transformer.h.0.ln_1.bias', 'base_model.model.transformer.h.0.attn.bias', 'base_model.model.transformer.h.0.attn.masked_bias', 'base_model.model.transformer.h.0.attn.c_attn.weight', 'base_model.model.transformer.h.0.attn.c_attn.bias', 'base_model.model.transformer.h.0.attn.c_proj.weight', 'base_model.model.transformer.h.0.attn.c_proj.bias', 'base_model.model.transformer.h.0.ln_2.weight', 'base_model.model.transformer.h.0.ln_2.bias', 'base_model.model.transformer.h.0.mlp.c_fc.weight', 'base_model.model.transformer.h.0.mlp.c_fc.bias', 'base_model.model.transformer.h.0.mlp.c_proj.weight', 'base_model.model.transformer.h.0.mlp.c_proj.bias', 'base_model.model.transformer.h.1.ln_1.weight', 'base_model.model.transformer.h.1.ln_1.bias', 'base_model.model.transformer.h.1.attn.bias', 'base_model.model.transformer.h.1.attn.masked_bias', 'base_model.model.transformer.h.1.attn.c_attn.weight', 'base_model.model.transformer.h.1.attn.c_attn.bias', 'base_model.model.transformer.h.1.attn.c_proj.weight', 'base_model.model.transformer.h.1.attn.c_proj.bias', 'base_model.model.transformer.h.1.ln_2.weight', 'base_model.model.transformer.h.1.ln_2.bias', 'base_model.model.transformer.h.1.mlp.c_fc.weight', 'base_model.model.transformer.h.1.mlp.c_fc.bias', 'base_model.model.transformer.h.1.mlp.c_proj.weight', 'base_model.model.transformer.h.1.mlp.c_proj.bias', 'base_model.model.transformer.h.2.ln_1.weight', 'base_model.model.transformer.h.2.ln_1.bias', 'base_model.model.transformer.h.2.attn.bias', 'base_model.model.transformer.h.2.attn.masked_bias', 'base_model.model.transformer.h.2.attn.c_attn.weight', 'base_model.model.transformer.h.2.attn.c_attn.bias', 'base_model.model.transformer.h.2.attn.c_proj.weight', 'base_model.model.transformer.h.2.attn.c_proj.bias', 'base_model.model.transformer.h.2.ln_2.weight', 'base_model.model.transformer.h.2.ln_2.bias', 'base_model.model.transformer.h.2.mlp.c_fc.weight', 'base_model.model.transformer.h.2.mlp.c_fc.bias', 'base_model.model.transformer.h.2.mlp.c_proj.weight', 'base_model.model.transformer.h.2.mlp.c_proj.bias', 'base_model.model.transformer.h.3.ln_1.weight', 'base_model.model.transformer.h.3.ln_1.bias', 'base_model.model.transformer.h.3.attn.bias', 'base_model.model.transformer.h.3.attn.masked_bias', 'base_model.model.transformer.h.3.attn.c_attn.weight', 'base_model.model.transformer.h.3.attn.c_attn.bias', 'base_model.model.transformer.h.3.attn.c_proj.weight', 'base_model.model.transformer.h.3.attn.c_proj.bias', 'base_model.model.transformer.h.3.ln_2.weight', 'base_model.model.transformer.h.3.ln_2.bias', 'base_model.model.transformer.h.3.mlp.c_fc.weight', 'base_model.model.transformer.h.3.mlp.c_fc.bias', 'base_model.model.transformer.h.3.mlp.c_proj.weight', 'base_model.model.transformer.h.3.mlp.c_proj.bias', 'base_model.model.transformer.h.4.ln_1.weight', 'base_model.model.transformer.h.4.ln_1.bias', 'base_model.model.transformer.h.4.attn.bias', 'base_model.model.transformer.h.4.attn.masked_bias', 'base_model.model.transformer.h.4.attn.c_attn.weight', 'base_model.model.transformer.h.4.attn.c_attn.bias', 'base_model.model.transformer.h.4.attn.c_proj.weight', 'base_model.model.transformer.h.4.attn.c_proj.bias', 'base_model.model.transformer.h.4.ln_2.weight', 'base_model.model.transformer.h.4.ln_2.bias', 'base_model.model.transformer.h.4.mlp.c_fc.weight', 'base_model.model.transformer.h.4.mlp.c_fc.bias', 'base_model.model.transformer.h.4.mlp.c_proj.weight', 'base_model.model.transformer.h.4.mlp.c_proj.bias', 'base_model.model.transformer.h.5.ln_1.weight', 'base_model.model.transformer.h.5.ln_1.bias', 'base_model.model.transformer.h.5.attn.bias', 'base_model.model.transformer.h.5.attn.masked_bias', 'base_model.model.transformer.h.5.attn.c_attn.weight', 'base_model.model.transformer.h.5.attn.c_attn.bias', 'base_model.model.transformer.h.5.attn.c_proj.weight', 'base_model.model.transformer.h.5.attn.c_proj.bias', 'base_model.model.transformer.h.5.ln_2.weight', 'base_model.model.transformer.h.5.ln_2.bias', 'base_model.model.transformer.h.5.mlp.c_fc.weight', 'base_model.model.transformer.h.5.mlp.c_fc.bias', 'base_model.model.transformer.h.5.mlp.c_proj.weight', 'base_model.model.transformer.h.5.mlp.c_proj.bias', 'base_model.model.transformer.h.6.ln_1.weight', 'base_model.model.transformer.h.6.ln_1.bias', 'base_model.model.transformer.h.6.attn.bias', 'base_model.model.transformer.h.6.attn.masked_bias', 'base_model.model.transformer.h.6.attn.c_attn.weight', 'base_model.model.transformer.h.6.attn.c_attn.bias', 'base_model.model.transformer.h.6.attn.c_proj.weight', 'base_model.model.transformer.h.6.attn.c_proj.bias', 'base_model.model.transformer.h.6.ln_2.weight', 'base_model.model.transformer.h.6.ln_2.bias', 'base_model.model.transformer.h.6.mlp.c_fc.weight', 'base_model.model.transformer.h.6.mlp.c_fc.bias', 'base_model.model.transformer.h.6.mlp.c_proj.weight', 'base_model.model.transformer.h.6.mlp.c_proj.bias', 'base_model.model.transformer.h.7.ln_1.weight', 'base_model.model.transformer.h.7.ln_1.bias', 'base_model.model.transformer.h.7.attn.bias', 'base_model.model.transformer.h.7.attn.masked_bias', 'base_model.model.transformer.h.7.attn.c_attn.weight', 'base_model.model.transformer.h.7.attn.c_attn.bias', 'base_model.model.transformer.h.7.attn.c_proj.weight', 'base_model.model.transformer.h.7.attn.c_proj.bias', 'base_model.model.transformer.h.7.ln_2.weight', 'base_model.model.transformer.h.7.ln_2.bias', 'base_model.model.transformer.h.7.mlp.c_fc.weight', 'base_model.model.transformer.h.7.mlp.c_fc.bias', 'base_model.model.transformer.h.7.mlp.c_proj.weight', 'base_model.model.transformer.h.7.mlp.c_proj.bias', 'base_model.model.transformer.h.8.ln_1.weight', 'base_model.model.transformer.h.8.ln_1.bias', 'base_model.model.transformer.h.8.attn.bias', 'base_model.model.transformer.h.8.attn.masked_bias', 'base_model.model.transformer.h.8.attn.c_attn.weight', 'base_model.model.transformer.h.8.attn.c_attn.bias', 'base_model.model.transformer.h.8.attn.c_proj.weight', 'base_model.model.transformer.h.8.attn.c_proj.bias', 'base_model.model.transformer.h.8.ln_2.weight', 'base_model.model.transformer.h.8.ln_2.bias', 'base_model.model.transformer.h.8.mlp.c_fc.weight', 'base_model.model.transformer.h.8.mlp.c_fc.bias', 'base_model.model.transformer.h.8.mlp.c_proj.weight', 'base_model.model.transformer.h.8.mlp.c_proj.bias', 'base_model.model.transformer.h.9.ln_1.weight', 'base_model.model.transformer.h.9.ln_1.bias', 'base_model.model.transformer.h.9.attn.bias', 'base_model.model.transformer.h.9.attn.masked_bias', 'base_model.model.transformer.h.9.attn.c_attn.weight', 'base_model.model.transformer.h.9.attn.c_attn.bias', 'base_model.model.transformer.h.9.attn.c_proj.weight', 'base_model.model.transformer.h.9.attn.c_proj.bias', 'base_model.model.transformer.h.9.ln_2.weight', 'base_model.model.transformer.h.9.ln_2.bias', 'base_model.model.transformer.h.9.mlp.c_fc.weight', 'base_model.model.transformer.h.9.mlp.c_fc.bias', 'base_model.model.transformer.h.9.mlp.c_proj.weight', 'base_model.model.transformer.h.9.mlp.c_proj.bias', 'base_model.model.transformer.h.10.ln_1.weight', 'base_model.model.transformer.h.10.ln_1.bias', 'base_model.model.transformer.h.10.attn.bias', 'base_model.model.transformer.h.10.attn.masked_bias', 'base_model.model.transformer.h.10.attn.c_attn.weight', 'base_model.model.transformer.h.10.attn.c_attn.bias', 'base_model.model.transformer.h.10.attn.c_proj.weight', 'base_model.model.transformer.h.10.attn.c_proj.bias', 'base_model.model.transformer.h.10.ln_2.weight', 'base_model.model.transformer.h.10.ln_2.bias', 'base_model.model.transformer.h.10.mlp.c_fc.weight', 'base_model.model.transformer.h.10.mlp.c_fc.bias', 'base_model.model.transformer.h.10.mlp.c_proj.weight', 'base_model.model.transformer.h.10.mlp.c_proj.bias', 'base_model.model.transformer.h.11.ln_1.weight', 'base_model.model.transformer.h.11.ln_1.bias', 'base_model.model.transformer.h.11.attn.bias', 'base_model.model.transformer.h.11.attn.masked_bias', 'base_model.model.transformer.h.11.attn.c_attn.weight', 'base_model.model.transformer.h.11.attn.c_attn.bias', 'base_model.model.transformer.h.11.attn.c_proj.weight', 'base_model.model.transformer.h.11.attn.c_proj.bias', 'base_model.model.transformer.h.11.ln_2.weight', 'base_model.model.transformer.h.11.ln_2.bias', 'base_model.model.transformer.h.11.mlp.c_fc.weight', 'base_model.model.transformer.h.11.mlp.c_fc.bias', 'base_model.model.transformer.h.11.mlp.c_proj.weight', 'base_model.model.transformer.h.11.mlp.c_proj.bias', 'base_model.model.transformer.ln_f.weight', 'base_model.model.transformer.ln_f.bias', 'base_model.model.lm_head.0.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " If there's a warning about missing keys above, please disregard :)\n",
      "11720\n"
     ]
    }
   ],
   "source": [
    "    df_train,df_val, df_test = get_data(\"bandgap\")\n",
    "    train(df_train, df_val, base_model=\"gpt2\", prop=\"bandgap\")\n",
    "    print(len(df_test))\n",
    "    outs=generate(df_test.head(1000), base_model=\"gpt2\", prop=\"bandgap\")\n",
    "    outs.to_json(\"outputs_gpt2_bandgap.json\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(outs['energy_true'], outs['energy_out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(outs['energy_true'], outs['energy_out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemGPT",
   "language": "python",
   "name": "chemgpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
