{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "from gptchem.gpt_classifier import GPTClassifier\n",
    "from gptchem.tuner import Tuner\n",
    "from gptchem.formatter import RegressionFormatter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pycm import ConfusionMatrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING = {\n",
    "    \"t5\": [\"q\", \"v\"],\n",
    "    \"mt5\": [\"q\", \"v\"],\n",
    "    \"bart\": [\"q_proj\", \"v_proj\"],\n",
    "    \"gpt2\": [\"c_attn\"],\n",
    "    \"bloom\": [\"query_key_value\"],\n",
    "    \"blip-2\": [\"q\", \"v\", \"q_proj\", \"v_proj\"],\n",
    "    \"opt\": [\"q_proj\", \"v_proj\"],\n",
    "    \"gptj\": [\"q_proj\", \"v_proj\"],\n",
    "    \"gpt_neox\": [\"query_key_value\"],\n",
    "    \"gpt_neo\": [\"q_proj\", \"v_proj\"],\n",
    "    \"bert\": [\"query\", \"value\"],\n",
    "    \"roberta\": [\"query\", \"value\"],\n",
    "    \"xlm-roberta\": [\"query\", \"value\"],\n",
    "    \"electra\": [\"query\", \"value\"],\n",
    "    \"deberta-v2\": [\"query_proj\", \"value_proj\"],\n",
    "    \"deberta\": [\"in_proj\"],\n",
    "    \"layoutlm\": [\"query\", \"value\"],\n",
    "    \"llama\": [\"q_proj\", \"v_proj\"],\n",
    "    \"chatglm\": [\"query_key_value\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from typing import List\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, EarlyStoppingCallback\n",
    "import os\n",
    "print(torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the data\n",
    "def get_data():\n",
    "    with open(\"qm9_key_smiles_0_val_u0_atom_b3lyp.pickle\", 'rb') as pickle_file:\n",
    "        pickled_data=pickle.load(pickle_file)\n",
    "    raw_data = pd.DataFrame(list(pickled_data.items()), columns=[\"SMILES\", \"B3LYP atomization energy in kcal/mol\"])\n",
    "    formatter = RegressionFormatter(representation_column='SMILES',\n",
    "        label_column='B3LYP atomization energy in kcal/mol',\n",
    "        property_name='atomization energy in kcal/mol',\n",
    "        num_digits=4\n",
    "        )\n",
    "    data = formatter.format_many(raw_data).drop(columns=[\"label\",\"representation\"], axis=1)\n",
    "    train_size=round(0.8*len(data))\n",
    "    val_size=round((len(data)-train_size)/2)\n",
    "    test_size=len(data)-train_size-val_size\n",
    "    df_trainval, df_test = train_test_split(data, test_size=val_size, train_size=train_size, random_state=42)\n",
    "    df_train, df_val = train_test_split(df_trainval,test_size=test_size, shuffle=True)\n",
    "\n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    #dataframes\n",
    "    df_train: pd.DataFrame,\n",
    "    df_val: pd.DataFrame,\n",
    "    # model/data params\n",
    "    base_model: str = \"gpt2\",  # the only required argument\n",
    "    data_path: str = \"qm9_key_smiles_0_val_u0_atom_b3lyp.pickle\",\n",
    "    output_dir: str = \"outputs\",\n",
    "    # training hyperparams\n",
    "    batch_size: int = 1024,\n",
    "    micro_batch_size: int = 64,\n",
    "    num_epochs: int = 10,\n",
    "    learning_rate: float = 3e-4,\n",
    "    cutoff_len: int = 256,\n",
    "    # lora hyperparams\n",
    "    lora_r: int = 8,\n",
    "    lora_alpha: int = 16,\n",
    "    lora_dropout: float = 0.05,\n",
    "    lora_target_modules: List[str] = [\"\"],\n",
    "    # llm hyperparams\n",
    "    train_on_inputs: bool = True,  # if False, masks out inputs in loss\n",
    "    group_by_length: bool = False,  # faster, but produces an odd training loss curve\n",
    "    resume_from_checkpoint: str = None,  # either training checkpoint or final adapter\n",
    "    prompt_template_name: str = \"chemgpt\",  # The prompt template to use, will default to alpaca.\n",
    "):\n",
    "    \n",
    "    lora_target_modules = TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING[base_model]\n",
    "    if int(os.environ.get(\"LOCAL_RANK\", 0)) == 0:\n",
    "        print(\n",
    "            f\"Training LoRA model with params:\\n\"\n",
    "            f\"base_model: {base_model}\\n\"\n",
    "            f\"data_path: {data_path}\\n\"\n",
    "            f\"output_dir: {output_dir}_{base_model}\\n\"\n",
    "            f\"batch_size: {batch_size}\\n\"\n",
    "            f\"micro_batch_size: {micro_batch_size}\\n\"\n",
    "            f\"num_epochs: {num_epochs}\\n\"\n",
    "            f\"learning_rate: {learning_rate}\\n\"\n",
    "            f\"cutoff_len: {cutoff_len}\\n\"\n",
    "            f\"lora_r: {lora_r}\\n\"\n",
    "            f\"lora_alpha: {lora_alpha}\\n\"\n",
    "            f\"lora_dropout: {lora_dropout}\\n\"\n",
    "            f\"lora_target_modules: {lora_target_modules}\\n\"\n",
    "            f\"train_on_inputs: {train_on_inputs}\\n\"\n",
    "            f\"group_by_length: {group_by_length}\\n\"\n",
    "            f\"resume_from_checkpoint: {resume_from_checkpoint or False}\\n\"\n",
    "            f\"prompt template: {prompt_template_name}\\n\"\n",
    "        )\n",
    "    assert (\n",
    "        base_model\n",
    "    ), \"Please specify a --base_model, e.g. --base_model='gpt2'\"\n",
    "    gradient_accumulation_steps = batch_size // micro_batch_size\n",
    "\n",
    "    device_map = \"sequential\"\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "    #ddp = world_size != 1\n",
    "    ddp = False\n",
    "    if ddp:\n",
    "        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
    "        gradient_accumulation_steps = gradient_accumulation_steps // world_size\n",
    "    \n",
    "    #set up the model and tokenizer    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "    # might not be optimal, just trying to run the code\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model, \n",
    "        load_in_8bit=False,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map='sequential',\n",
    "    )    \n",
    "    def tokenize(prompt):\n",
    "        result = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=cutoff_len,\n",
    "            padding=True,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        return result\n",
    "    \n",
    "    def tokenize_prompt(data_point):\n",
    "        full_prompt = data_point[\"prompt\"]+data_point[\"completion\"]\n",
    "        tokenized_full_prompt = tokenize(full_prompt)\n",
    "        return tokenized_full_prompt\n",
    "\n",
    "    model = prepare_model_for_int8_training(model)\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=lora_target_modules,\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, config)\n",
    "\n",
    "    train_data = Dataset.from_pandas(df_train).shuffle().map(tokenize_prompt)\n",
    "    val_data = Dataset.from_pandas(df_val).shuffle().map(tokenize_prompt)\n",
    "    test_data = Dataset.from_pandas(df_test).shuffle().map(tokenize_prompt)\n",
    "    \n",
    "    if resume_from_checkpoint:\n",
    "        # Check the available weights and load them\n",
    "        checkpoint_name = os.path.join(\n",
    "            resume_from_checkpoint, \"pytorch_model.bin\"\n",
    "        )  # Full checkpoint\n",
    "        if not os.path.exists(checkpoint_name):\n",
    "            checkpoint_name = os.path.join(\n",
    "                resume_from_checkpoint, \"adapter_model.bin\"\n",
    "            )  # only LoRA model - LoRA config above has to fit\n",
    "            resume_from_checkpoint = (\n",
    "                False  # So the trainer won't try loading its state\n",
    "            )\n",
    "        # The two files above have a different name depending on how they were saved, but are actually the same.\n",
    "        if os.path.exists(checkpoint_name):\n",
    "            print(f\"Restarting from {checkpoint_name}\")\n",
    "            adapters_weights = torch.load(checkpoint_name)\n",
    "            model = set_peft_model_state_dict(model, adapters_weights)\n",
    "        else:\n",
    "            print(f\"Checkpoint {checkpoint_name} not found\")\n",
    "\n",
    "    model.print_trainable_parameters()  # Be more transparent about the % of trainable params.\n",
    "    \n",
    "    if not ddp and torch.cuda.device_count() > 1:\n",
    "        # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n",
    "        model.is_parallelizable = True\n",
    "        model.model_parallel = True\n",
    "   \n",
    "    trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        args=transformers.TrainingArguments(\n",
    "            per_device_train_batch_size=micro_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            warmup_steps=10,\n",
    "            num_train_epochs=num_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            fp16=True,\n",
    "            logging_steps=4,\n",
    "            optim=\"adamw_torch\",\n",
    "            evaluation_strategy=\"steps\",\n",
    "            save_strategy=\"steps\",\n",
    "            eval_steps=4,\n",
    "            save_steps=4,\n",
    "            output_dir=output_dir+\"_\"+base_model,\n",
    "            save_total_limit=3,\n",
    "            metric_for_best_model = 'eval_loss',\n",
    "            load_best_model_at_end=True,\n",
    "            ddp_find_unused_parameters=False if ddp else None,\n",
    "            group_by_length=group_by_length,\n",
    "        ),\n",
    "        callbacks = [EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "        data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "    )\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    old_state_dict = model.state_dict\n",
    "    print(old_state_dict)\n",
    "    print(model.state_dict)\n",
    "    model.state_dict = (\n",
    "        lambda self, *_, **__: get_peft_model_state_dict(\n",
    "            self, old_state_dict()\n",
    "        )\n",
    "    ).__get__(model, type(model))\n",
    "    \n",
    "    \n",
    "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "\n",
    "    model.save_pretrained(output_dir+\"_\"+base_model)\n",
    "\n",
    "    print(\n",
    "        \"\\n If there's a warning about missing keys above, please disregard :)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import GenerationConfig\n",
    "#from utils.callbacks import Iteratorize, Stream\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\" \n",
    "    \n",
    "def generate(\n",
    "    df_test: pd.DataFrame,\n",
    "    load_8bit: bool = False,\n",
    "    base_model: str = \"gpt2\",\n",
    "    lora_weights: str = \"outputs\",\n",
    "    prompt_template: str = \"\",\n",
    "    cutoff_len: int = 256,\n",
    "):\n",
    "    #set up the model and tokenizer    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "    # might not be optimal, just trying to run the code\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    def tokenize(prompt):\n",
    "        result = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=cutoff_len,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    model= AutoModelForCausalLM.from_pretrained(\n",
    "        base_model, \n",
    "        load_in_8bit=False,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map='sequential',\n",
    "    )    \n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        lora_weights+\"_\"+base_model,\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    if not load_8bit:\n",
    "        model.half()  # seems to fix bugs for some users.\n",
    "\n",
    "    model.eval()\n",
    "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    def evaluate(\n",
    "        prompt,\n",
    "        temperature=0,\n",
    "        top_p=0.75,\n",
    "        top_k=40,\n",
    "        num_beams=4,\n",
    "        max_new_tokens=128,\n",
    "        stream_output=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        inputs = tokenize(prompt)\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        generation_config = GenerationConfig(\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            num_beams=num_beams,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        generate_params = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"generation_config\": generation_config,\n",
    "            \"return_dict_in_generate\": True,\n",
    "            \"output_scores\": True,\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "        }\n",
    "\n",
    "        # Without streaming\n",
    "        with torch.no_grad():\n",
    "            generation_output = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                generation_config=generation_config,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "            )\n",
    "        s = generation_output.sequences[0]\n",
    "        output = tokenizer.decode(s)\n",
    "        #print(output)\n",
    "        return output\n",
    "\n",
    "    df_test[\"model_out\"] = df_test[\"prompt\"].map(lambda x: evaluate(x))\n",
    "    df_test[\"energy_out\"] = df_test[\"model_out\"].map(lambda x: float(x.replace('###','@@@').split('@@@')[1]))\n",
    "    df_test[\"energy_true\"] = df_test[\"completion\"].map(lambda x: float(x.split('@@@')[0]))\n",
    "    return(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LoRA model with params:\n",
      "base_model: gpt2\n",
      "data_path: qm9_key_smiles_0_val_u0_atom_b3lyp.pickle\n",
      "output_dir: outputs_gpt2\n",
      "batch_size: 1024\n",
      "micro_batch_size: 64\n",
      "num_epochs: 10\n",
      "learning_rate: 0.0003\n",
      "cutoff_len: 256\n",
      "lora_r: 8\n",
      "lora_alpha: 16\n",
      "lora_dropout: 0.05\n",
      "lora_target_modules: ['c_attn']\n",
      "train_on_inputs: True\n",
      "group_by_length: False\n",
      "resume_from_checkpoint: False\n",
      "prompt template: chemgpt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      2\u001b[0m     df_train, df_val, df_test \u001b[38;5;241m=\u001b[39m get_data()\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     outs\u001b[38;5;241m=\u001b[39mgenerate(df_test\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m25\u001b[39m))\n",
      "Cell \u001b[0;32mIn[41], line 99\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(df_train, df_val, base_model, data_path, output_dir, batch_size, micro_batch_size, num_epochs, learning_rate, cutoff_len, lora_r, lora_alpha, lora_dropout, lora_target_modules, train_on_inputs, group_by_length, resume_from_checkpoint, prompt_template_name)\u001b[0m\n\u001b[1;32m     89\u001b[0m config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[1;32m     90\u001b[0m     r\u001b[38;5;241m=\u001b[39mlora_r,\n\u001b[1;32m     91\u001b[0m     lora_alpha\u001b[38;5;241m=\u001b[39mlora_alpha,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m     task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAUSAL_LM\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     96\u001b[0m )\n\u001b[1;32m     97\u001b[0m model \u001b[38;5;241m=\u001b[39m get_peft_model(model, config)\n\u001b[0;32m---> 99\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m val_data \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(df_val)\u001b[38;5;241m.\u001b[39mshuffle()\u001b[38;5;241m.\u001b[39mmap(tokenize_prompt)\n\u001b[1;32m    101\u001b[0m test_data \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(df_test)\u001b[38;5;241m.\u001b[39mshuffle()\u001b[38;5;241m.\u001b[39mmap(tokenize_prompt)\n",
      "File \u001b[0;32m~/miniconda3/envs/chemGPT/lib/python3.10/site-packages/datasets/arrow_dataset.py:563\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 563\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/chemGPT/lib/python3.10/site-packages/datasets/arrow_dataset.py:528\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    521\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    526\u001b[0m }\n\u001b[1;32m    527\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 528\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    529\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    530\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/chemGPT/lib/python3.10/site-packages/datasets/arrow_dataset.py:3004\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   2996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2997\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mtqdm(\n\u001b[1;32m   2998\u001b[0m         disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   2999\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3002\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3003\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3004\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3005\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3006\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/chemGPT/lib/python3.10/site-packages/datasets/arrow_dataset.py:3358\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3356\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3357\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3358\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[1;32m   3360\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/chemGPT/lib/python3.10/site-packages/datasets/arrow_dataset.py:3261\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3260\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3261\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3263\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3264\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3265\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[41], line 83\u001b[0m, in \u001b[0;36mtrain.<locals>.tokenize_prompt\u001b[0;34m(data_point)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_prompt\u001b[39m(data_point):\n\u001b[0;32m---> 83\u001b[0m     full_prompt \u001b[38;5;241m=\u001b[39m \u001b[43mdata_point\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m+\u001b[39mdata_point[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     84\u001b[0m     tokenized_full_prompt \u001b[38;5;241m=\u001b[39m tokenize(full_prompt)\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_full_prompt\n",
      "File \u001b[0;32m~/miniconda3/envs/chemGPT/lib/python3.10/site-packages/datasets/formatting/formatting.py:284\u001b[0m, in \u001b[0;36mLazyDict.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    282\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(key)\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys_to_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df_train, df_val, df_test = get_data()\n",
    "    train(df_train, df_val)\n",
    "    outs=generate(df_test.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "      <th>model_out</th>\n",
       "      <th>energy_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74606</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-2071.9989@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56992</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1890.364@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16367</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1361.6393@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1653.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12238</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1466.7489@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42893</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1591.6076@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13203</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1405.7739@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82846</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1841.7596@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25217</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1599.1101@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47472</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1743.1485@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116647</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1798.0185@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104737</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1869.2652@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1653.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12947</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1483.211@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12181</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1784.618@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121548</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1607.9761@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65617</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1759.5296@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13238</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-2191.8958@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18843</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1741.6192@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1753.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41836</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1492.522@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118167</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1979.7081@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22273</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1627.3498@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46552</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-2055.0035@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122615</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1431.7735@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96842</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-2068.9536@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42403</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1943.0791@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98367</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1913.2166@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   prompt      completion  \\\n",
       "74606   What is the atomization energy in kcal/mol of ...   -2071.9989@@@   \n",
       "56992   What is the atomization energy in kcal/mol of ...    -1890.364@@@   \n",
       "16367   What is the atomization energy in kcal/mol of ...   -1361.6393@@@   \n",
       "12238   What is the atomization energy in kcal/mol of ...   -1466.7489@@@   \n",
       "42893   What is the atomization energy in kcal/mol of ...   -1591.6076@@@   \n",
       "13203   What is the atomization energy in kcal/mol of ...   -1405.7739@@@   \n",
       "82846   What is the atomization energy in kcal/mol of ...   -1841.7596@@@   \n",
       "25217   What is the atomization energy in kcal/mol of ...   -1599.1101@@@   \n",
       "47472   What is the atomization energy in kcal/mol of ...   -1743.1485@@@   \n",
       "116647  What is the atomization energy in kcal/mol of ...   -1798.0185@@@   \n",
       "104737  What is the atomization energy in kcal/mol of ...   -1869.2652@@@   \n",
       "12947   What is the atomization energy in kcal/mol of ...    -1483.211@@@   \n",
       "12181   What is the atomization energy in kcal/mol of ...    -1784.618@@@   \n",
       "121548  What is the atomization energy in kcal/mol of ...   -1607.9761@@@   \n",
       "65617   What is the atomization energy in kcal/mol of ...   -1759.5296@@@   \n",
       "13238   What is the atomization energy in kcal/mol of ...   -2191.8958@@@   \n",
       "18843   What is the atomization energy in kcal/mol of ...   -1741.6192@@@   \n",
       "41836   What is the atomization energy in kcal/mol of ...    -1492.522@@@   \n",
       "118167  What is the atomization energy in kcal/mol of ...   -1979.7081@@@   \n",
       "22273   What is the atomization energy in kcal/mol of ...   -1627.3498@@@   \n",
       "46552   What is the atomization energy in kcal/mol of ...   -2055.0035@@@   \n",
       "122615  What is the atomization energy in kcal/mol of ...   -1431.7735@@@   \n",
       "96842   What is the atomization energy in kcal/mol of ...   -2068.9536@@@   \n",
       "42403   What is the atomization energy in kcal/mol of ...   -1943.0791@@@   \n",
       "98367   What is the atomization energy in kcal/mol of ...   -1913.2166@@@   \n",
       "\n",
       "                                                model_out  energy_out  \n",
       "74606   What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "56992   What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "16367   What is the atomization energy in kcal/mol of ...   -1653.906  \n",
       "12238   What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "42893   What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "13203   What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "82846   What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "25217   What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "47472   What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "116647  What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "104737  What is the atomization energy in kcal/mol of ...   -1653.906  \n",
       "12947   What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "12181   What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "121548  What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "65617   What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "13238   What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "18843   What is the atomization energy in kcal/mol of ...   -1753.906  \n",
       "41836   What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "118167  What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "22273   What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "46552   What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "122615  What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "96842   What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "42403   What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "98367   What is the atomization energy in kcal/mol of ...   -1853.906  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'energy_true'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/chemGPT/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/envs/chemGPT/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/chemGPT/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'energy_true'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(\u001b[43mouts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43menergy_true\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, outs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menergy_out\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/chemGPT/lib/python3.10/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/envs/chemGPT/lib/python3.10/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'energy_true'"
     ]
    }
   ],
   "source": [
    "plt.scatter(outs['energy_true'], outs['energy_out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(outs['energy_true'], outs['energy_out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemGPT",
   "language": "python",
   "name": "chemgpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
