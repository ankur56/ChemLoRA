{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/g/m/gmerz2/miniconda3/envs/chemGPT/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "from gptchem.gpt_classifier import GPTClassifier\n",
    "from gptchem.tuner import Tuner\n",
    "from gptchem.formatter import RegressionFormatter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pycm import ConfusionMatrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING = {\n",
    "    \"t5\": [\"q\", \"v\"],\n",
    "    \"mt5\": [\"q\", \"v\"],\n",
    "    \"bart\": [\"q_proj\", \"v_proj\"],\n",
    "    \"gpt2\": [\"c_attn\"],\n",
    "    \"bloom\": [\"query_key_value\"],\n",
    "    \"blip-2\": [\"q\", \"v\", \"q_proj\", \"v_proj\"],\n",
    "    \"opt\": [\"q_proj\", \"v_proj\"],\n",
    "    \"gptj\": [\"q_proj\", \"v_proj\"],\n",
    "    \"gpt_neox\": [\"query_key_value\"],\n",
    "    \"gpt_neo\": [\"q_proj\", \"v_proj\"],\n",
    "    \"bert\": [\"query\", \"value\"],\n",
    "    \"roberta\": [\"query\", \"value\"],\n",
    "    \"xlm-roberta\": [\"query\", \"value\"],\n",
    "    \"electra\": [\"query\", \"value\"],\n",
    "    \"deberta-v2\": [\"query_proj\", \"value_proj\"],\n",
    "    \"deberta\": [\"in_proj\"],\n",
    "    \"layoutlm\": [\"query\", \"value\"],\n",
    "    \"llama\": [\"q_proj\", \"v_proj\"],\n",
    "    \"chatglm\": [\"query_key_value\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-10 15:07:00.248980: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-10 15:07:06.840439: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 121\n",
      "CUDA SETUP: Loading binary /u/g/m/gmerz2/miniconda3/envs/chemGPT/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/g/m/gmerz2/miniconda3/envs/chemGPT/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /u/g/m/gmerz2/miniconda3 did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/u/g/m/gmerz2/miniconda3/envs/chemGPT/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {Path('FILE')}\n",
      "  warn(msg)\n",
      "/u/g/m/gmerz2/miniconda3/envs/chemGPT/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {Path('/run/user/2289/docker.sock'), Path('unix')}\n",
      "  warn(msg)\n",
      "/u/g/m/gmerz2/miniconda3/envs/chemGPT/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {Path('//matplotlib_inline.backend_inline'), Path('module')}\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "import fire\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the data\n",
    "def get_data():\n",
    "    with open(\"qm9_key_smiles_0_val_u0_atom_b3lyp.pickle\", 'rb') as pickle_file:\n",
    "        pickled_data=pickle.load(pickle_file)\n",
    "    raw_data = pd.DataFrame(list(pickled_data.items()), columns=[\"SMILES\", \"B3LYP atomization energy in kcal/mol\"])\n",
    "    formatter = RegressionFormatter(representation_column='SMILES',\n",
    "        label_column='B3LYP atomization energy in kcal/mol',\n",
    "        property_name='atomization energy in kcal/mol',\n",
    "        num_digits=4\n",
    "        )\n",
    "    data = formatter.format_many(raw_data).drop(columns=[\"label\",\"representation\"], axis=1)\n",
    "    df_trainval, df_test = train_test_split(data, test_size=100, train_size=900, random_state=42)\n",
    "    df_train, df_val = train_test_split(df_trainval,test_size=100, shuffle=True)\n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    #dataframes\n",
    "    df_train: pd.DataFrame,\n",
    "    df_val: pd.DataFrame,\n",
    "    # model/data params\n",
    "    base_model: str = \"gpt2\",  # the only required argument\n",
    "    data_path: str = \"qm9_key_smiles_0_val_u0_atom_b3lyp.pickle\",\n",
    "    output_dir: str = \"outputs\",\n",
    "    # training hyperparams\n",
    "    batch_size: int = 128,\n",
    "    micro_batch_size: int = 4,\n",
    "    num_epochs: int = 3,\n",
    "    learning_rate: float = 3e-4,\n",
    "    cutoff_len: int = 256,\n",
    "    # lora hyperparams\n",
    "    lora_r: int = 8,\n",
    "    lora_alpha: int = 16,\n",
    "    lora_dropout: float = 0.05,\n",
    "    lora_target_modules: List[str] = [\"\"],\n",
    "    # llm hyperparams\n",
    "    train_on_inputs: bool = True,  # if False, masks out inputs in loss\n",
    "    group_by_length: bool = False,  # faster, but produces an odd training loss curve\n",
    "    # wandb params\n",
    "    wandb_project: str = \"\",\n",
    "    wandb_run_name: str = \"\",\n",
    "    wandb_watch: str = \"\",  # options: false | gradients | all\n",
    "    wandb_log_model: str = \"\",  # options: false | true\n",
    "    resume_from_checkpoint: str = None,  # either training checkpoint or final adapter\n",
    "    prompt_template_name: str = \"chemgpt\",  # The prompt template to use, will default to alpaca.\n",
    "):\n",
    "    \n",
    "    lora_target_modules = TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING[base_model]\n",
    "    if int(os.environ.get(\"LOCAL_RANK\", 0)) == 0:\n",
    "        print(\n",
    "            f\"Training LoRA model with params:\\n\"\n",
    "            f\"base_model: {base_model}\\n\"\n",
    "            f\"data_path: {data_path}\\n\"\n",
    "            f\"output_dir: {output_dir}\\n\"\n",
    "            f\"batch_size: {batch_size}\\n\"\n",
    "            f\"micro_batch_size: {micro_batch_size}\\n\"\n",
    "            f\"num_epochs: {num_epochs}\\n\"\n",
    "            f\"learning_rate: {learning_rate}\\n\"\n",
    "            f\"cutoff_len: {cutoff_len}\\n\"\n",
    "            f\"lora_r: {lora_r}\\n\"\n",
    "            f\"lora_alpha: {lora_alpha}\\n\"\n",
    "            f\"lora_dropout: {lora_dropout}\\n\"\n",
    "            f\"lora_target_modules: {lora_target_modules}\\n\"\n",
    "            f\"train_on_inputs: {train_on_inputs}\\n\"\n",
    "            f\"group_by_length: {group_by_length}\\n\"\n",
    "            f\"wandb_project: {wandb_project}\\n\"\n",
    "            f\"wandb_run_name: {wandb_run_name}\\n\"\n",
    "            f\"wandb_watch: {wandb_watch}\\n\"\n",
    "            f\"wandb_log_model: {wandb_log_model}\\n\"\n",
    "            f\"resume_from_checkpoint: {resume_from_checkpoint or False}\\n\"\n",
    "            f\"prompt template: {prompt_template_name}\\n\"\n",
    "        )\n",
    "    assert (\n",
    "        base_model\n",
    "    ), \"Please specify a --base_model, e.g. --base_model='gpt2'\"\n",
    "    gradient_accumulation_steps = batch_size // micro_batch_size\n",
    "\n",
    "    device_map = \"auto\"\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "    ddp = world_size != 1\n",
    "    if ddp:\n",
    "        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
    "        gradient_accumulation_steps = gradient_accumulation_steps // world_size\n",
    "\n",
    "    # Check if parameter passed or if set within environ\n",
    "    use_wandb = len(wandb_project) > 0 or (\n",
    "        \"WANDB_PROJECT\" in os.environ and len(os.environ[\"WANDB_PROJECT\"]) > 0\n",
    "    )\n",
    "    # Only overwrite environ if wandb param passed\n",
    "    if len(wandb_project) > 0:\n",
    "        os.environ[\"WANDB_PROJECT\"] = wandb_project\n",
    "    if len(wandb_watch) > 0:\n",
    "        os.environ[\"WANDB_WATCH\"] = wandb_watch\n",
    "    if len(wandb_log_model) > 0:\n",
    "        os.environ[\"WANDB_LOG_MODEL\"] = wandb_log_model\n",
    "\n",
    "    #set up the model and tokenizer    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "    # might not be optimal, just trying to run the code\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model, \n",
    "        load_in_8bit=False,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map='auto',\n",
    "    )    \n",
    "    def tokenize(prompt):\n",
    "        result = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=cutoff_len,\n",
    "            padding=True,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        return result\n",
    "    \n",
    "    def tokenize_prompt(data_point):\n",
    "        full_prompt = data_point[\"prompt\"]+data_point[\"completion\"]\n",
    "        tokenized_full_prompt = tokenize(full_prompt)\n",
    "        return tokenized_full_prompt\n",
    "\n",
    "    model = prepare_model_for_int8_training(model)\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=lora_target_modules,\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, config)\n",
    "\n",
    "    train_data = Dataset.from_pandas(df_train).shuffle().map(tokenize_prompt)\n",
    "    val_data = Dataset.from_pandas(df_val).shuffle().map(tokenize_prompt)\n",
    "    test_data = Dataset.from_pandas(df_test).shuffle().map(tokenize_prompt)\n",
    "    \n",
    "    if resume_from_checkpoint:\n",
    "        # Check the available weights and load them\n",
    "        checkpoint_name = os.path.join(\n",
    "            resume_from_checkpoint, \"pytorch_model.bin\"\n",
    "        )  # Full checkpoint\n",
    "        if not os.path.exists(checkpoint_name):\n",
    "            checkpoint_name = os.path.join(\n",
    "                resume_from_checkpoint, \"adapter_model.bin\"\n",
    "            )  # only LoRA model - LoRA config above has to fit\n",
    "            resume_from_checkpoint = (\n",
    "                False  # So the trainer won't try loading its state\n",
    "            )\n",
    "        # The two files above have a different name depending on how they were saved, but are actually the same.\n",
    "        if os.path.exists(checkpoint_name):\n",
    "            print(f\"Restarting from {checkpoint_name}\")\n",
    "            adapters_weights = torch.load(checkpoint_name)\n",
    "            model = set_peft_model_state_dict(model, adapters_weights)\n",
    "        else:\n",
    "            print(f\"Checkpoint {checkpoint_name} not found\")\n",
    "\n",
    "    model.print_trainable_parameters()  # Be more transparent about the % of trainable params.\n",
    "    \n",
    "    if not ddp and torch.cuda.device_count() > 1:\n",
    "        # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n",
    "        model.is_parallelizable = True\n",
    "        model.model_parallel = True\n",
    "   \n",
    "    trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        args=transformers.TrainingArguments(\n",
    "            per_device_train_batch_size=micro_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            warmup_steps=10,\n",
    "            num_train_epochs=num_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            fp16=True,\n",
    "            logging_steps=4,\n",
    "            optim=\"adamw_torch\",\n",
    "            evaluation_strategy=\"steps\",\n",
    "            save_strategy=\"steps\",\n",
    "            eval_steps=4,\n",
    "            save_steps=4,\n",
    "            output_dir=output_dir,\n",
    "            save_total_limit=3,\n",
    "            load_best_model_at_end=True,\n",
    "            ddp_find_unused_parameters=False if ddp else None,\n",
    "            group_by_length=group_by_length,\n",
    "            report_to=\"wandb\" if use_wandb else None,\n",
    "            run_name=wandb_run_name if use_wandb else None,\n",
    "        ),\n",
    "        data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "    )\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    old_state_dict = model.state_dict\n",
    "    print(old_state_dict)\n",
    "    print(model.state_dict)\n",
    "    model.state_dict = (\n",
    "        lambda self, *_, **__: get_peft_model_state_dict(\n",
    "            self, old_state_dict()\n",
    "        )\n",
    "    ).__get__(model, type(model))\n",
    "    \n",
    "    \n",
    "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "\n",
    "    model.save_pretrained(output_dir)\n",
    "\n",
    "    print(\n",
    "        \"\\n If there's a warning about missing keys above, please disregard :)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "\n",
    "#from utils.callbacks import Iteratorize, Stream\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "def generate(\n",
    "    df_test: pd.DataFrame,\n",
    "    load_8bit: bool = False,\n",
    "    base_model: str = \"gpt2\",\n",
    "    lora_weights: str = \"outputs/checkpoint-16\",\n",
    "    prompt_template: str = \"\",\n",
    "):\n",
    "    #set up the model and tokenizer    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "    # might not be optimal, just trying to run the code\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    def tokenize(prompt):\n",
    "        result = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=cutoff_len,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    def tokenize_full_prompt(data_point):\n",
    "        full_prompt = data_point[\"prompt\"]+data_point[\"completion\"]\n",
    "        tokenized_full_prompt = tokenize(full_prompt)\n",
    "        return tokenized_full_prompt\n",
    "    \n",
    "    def tokenize_prompt(data_point):\n",
    "        prompt = data_point[\"prompt\"]\n",
    "        tokenized_prompt = tokenize(prompt)\n",
    "        return tokenized_prompt\n",
    "    \n",
    "    model= AutoModelForCausalLM.from_pretrained(\n",
    "        base_model, \n",
    "        load_in_8bit=False,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map='auto',\n",
    "    )    \n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        lora_weights,\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    \"\"\"    \n",
    "    # unwind broken decapoda-research config\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "    model.config.bos_token_id = 1\n",
    "    model.config.eos_token_id = 2\n",
    "\n",
    "    if not load_8bit:\n",
    "        model.half()  # seems to fix bugs for some users.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    def evaluate(\n",
    "        prompt,\n",
    "        temperature=0.1,\n",
    "        top_p=0.75,\n",
    "        top_k=40,\n",
    "        num_beams=4,\n",
    "        max_new_tokens=128,\n",
    "        stream_output=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        inputs = tokenize_prompt(prompt)\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        generation_config = GenerationConfig(\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            num_beams=num_beams,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        generate_params = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"generation_config\": generation_config,\n",
    "            \"return_dict_in_generate\": True,\n",
    "            \"output_scores\": True,\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "        }\n",
    "\n",
    "        \"\"\"\n",
    "        if stream_output:\n",
    "            # Stream the reply 1 token at a time.\n",
    "            # This is based on the trick of using 'stopping_criteria' to create an iterator,\n",
    "            # from https://github.com/oobabooga/text-generation-webui/blob/ad37f396fc8bcbab90e11ecf17c56c97bfbd4a9c/modules/text_generation.py#L216-L243.\n",
    "\n",
    "            def generate_with_callback(callback=None, **kwargs):\n",
    "                kwargs.setdefault(\n",
    "                    \"stopping_criteria\", transformers.StoppingCriteriaList()\n",
    "                )\n",
    "                kwargs[\"stopping_criteria\"].append(\n",
    "                    Stream(callback_func=callback)\n",
    "                )\n",
    "                with torch.no_grad():\n",
    "                    model.generate(**kwargs)\n",
    "\n",
    "            def generate_with_streaming(**kwargs):\n",
    "                return Iteratorize(\n",
    "                    generate_with_callback, kwargs, callback=None\n",
    "                )\n",
    "\n",
    "            with generate_with_streaming(**generate_params) as generator:\n",
    "                for output in generator:\n",
    "                    # new_tokens = len(output) - len(input_ids[0])\n",
    "                    decoded_output = tokenizer.decode(output)\n",
    "\n",
    "                    if output[-1] in [tokenizer.eos_token_id]:\n",
    "                        break\n",
    "\n",
    "                    yield decoded_output\n",
    "            return  # early return for stream_output\n",
    "        \"\"\"\n",
    "        # Without streaming\n",
    "        with torch.no_grad():\n",
    "            generation_output = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                generation_config=generation_config,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "            )\n",
    "        s = generation_output.sequences[0]\n",
    "        output = tokenizer.decode(s)\n",
    "        print(output)\n",
    "        yield output\n",
    "\n",
    "    test_data = Dataset.from_pandas(df_test).map(evaluate)        \n",
    "        \n",
    "    # Old testing code follows.\n",
    "\n",
    "    \"\"\"\n",
    "    # testing code for readme\n",
    "    for instruction in [\n",
    "        \"Tell me about alpacas.\",\n",
    "        \"Tell me about the president of Mexico in 2019.\",\n",
    "        \"Tell me about the king of France in 2019.\",\n",
    "        \"List all Canadian provinces in alphabetical order.\",\n",
    "        \"Write a Python program that prints the first 10 Fibonacci numbers.\",\n",
    "        \"Write a program that prints the numbers from 1 to 100. But for multiples of three print 'Fizz' instead of the number and for the multiples of five print 'Buzz'. For numbers which are multiples of both three and five print 'FizzBuzz'.\",  # noqa: E501\n",
    "        \"Tell me five words that rhyme with 'shock'.\",\n",
    "        \"Translate the sentence 'I have no mouth but I must scream' into Spanish.\",\n",
    "        \"Count up from 1 to 500.\",\n",
    "    ]:\n",
    "        print(\"Instruction:\", instruction)\n",
    "        print(\"Response:\", evaluate(instruction))\n",
    "        print()\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LoRA model with params:\n",
      "base_model: gpt2\n",
      "data_path: qm9_key_smiles_0_val_u0_atom_b3lyp.pickle\n",
      "output_dir: outputs\n",
      "batch_size: 128\n",
      "micro_batch_size: 4\n",
      "num_epochs: 3\n",
      "learning_rate: 0.0003\n",
      "cutoff_len: 256\n",
      "lora_r: 8\n",
      "lora_alpha: 16\n",
      "lora_dropout: 0.05\n",
      "lora_target_modules: ['c_attn']\n",
      "train_on_inputs: True\n",
      "group_by_length: False\n",
      "wandb_project: \n",
      "wandb_run_name: \n",
      "wandb_watch: \n",
      "wandb_log_model: \n",
      "resume_from_checkpoint: False\n",
      "prompt template: chemgpt\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      2\u001b[0m     df_train, df_val, df_test \u001b[38;5;241m=\u001b[39m get_data()\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     generate(df_test)\n",
      "Cell \u001b[0;32mIn[5], line 86\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(df_train, df_val, base_model, data_path, output_dir, batch_size, micro_batch_size, num_epochs, learning_rate, cutoff_len, lora_r, lora_alpha, lora_dropout, lora_target_modules, train_on_inputs, group_by_length, wandb_project, wandb_run_name, wandb_watch, wandb_log_model, resume_from_checkpoint, prompt_template_name)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# might not be optimal, just trying to run the code\u001b[39;00m\n\u001b[1;32m     84\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n\u001b[0;32m---> 86\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(prompt):\n\u001b[1;32m     93\u001b[0m     result \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[1;32m     94\u001b[0m         prompt,\n\u001b[1;32m     95\u001b[0m         truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     99\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/chemGPT/lib/python3.10/site-packages/transformers-4.27.4-py3.10.egg/transformers/models/auto/auto_factory.py:471\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    470\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    477\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/chemGPT/lib/python3.10/site-packages/transformers-4.27.4-py3.10.egg/transformers/modeling_utils.py:2575\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2570\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   2571\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis model has some weights that should be kept in higher precision, you need to upgrade \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2572\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`accelerate` to properly deal with them (`pip install --upgrade accelerate`).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2573\u001b[0m     )\n\u001b[1;32m   2574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequential\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m get_balanced_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2575\u001b[0m     max_memory \u001b[38;5;241m=\u001b[39m \u001b[43mget_balanced_memory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_zero\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbalanced_low_0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2580\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2581\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2582\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_memory\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m max_memory\n\u001b[1;32m   2583\u001b[0m \u001b[38;5;66;03m# Make sure tied weights are tied before creating the device map.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/chemGPT/lib/python3.10/site-packages/accelerate-0.18.0-py3.10.egg/accelerate/utils/modeling.py:465\u001b[0m, in \u001b[0;36mget_balanced_memory\u001b[0;34m(model, max_memory, no_split_module_classes, dtype, special_dtypes, low_zero)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;124;03mCompute a `max_memory` dictionary for [`infer_auto_device_map`] that will balance the use of each available GPU.\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;124;03m        Transformers generate function).\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;66;03m# Get default / clean up max_memory\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m max_memory \u001b[38;5;241m=\u001b[39m \u001b[43mget_max_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m max_memory\n",
      "File \u001b[0;32m~/miniconda3/envs/chemGPT/lib/python3.10/site-packages/accelerate-0.18.0-py3.10.egg/accelerate/utils/modeling.py:373\u001b[0m, in \u001b[0;36mget_max_memory\u001b[0;34m(max_memory)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;66;03m# Make sure CUDA is initialized on each GPU to have the right memory info.\u001b[39;00m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()):\n\u001b[0;32m--> 373\u001b[0m         _ \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m     max_memory \u001b[38;5;241m=\u001b[39m {i: torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmem_get_info(i)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count())}\n\u001b[1;32m    375\u001b[0m max_memory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m psutil\u001b[38;5;241m.\u001b[39mvirtual_memory()\u001b[38;5;241m.\u001b[39mavailable\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df_train, df_val, df_test = get_data()\n",
    "    train(df_train, df_val)\n",
    "    generate(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(df_test['representation'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ConfusionMatrix(df_test['bin'].tolist(), list(predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptchem.gpt_regressor import GPTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?GPTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = GPTRegressor('atomization energy', tuner=Tuner(n_epochs=8, learning_rate_multiplier=0.02, wandb_sync=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(df_train['representation'].values, df_train['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = regressor.predict(df_test['representation'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2ab7adca0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(df_test['label'].values, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PearsonRResult(statistic=0.28480027668863434, pvalue=0.004079810901305013)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(df_test['label'].values, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemGPT",
   "language": "python",
   "name": "chemgpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
