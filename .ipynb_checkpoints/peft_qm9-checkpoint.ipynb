{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "from gptchem.gpt_classifier import GPTClassifier\n",
    "from gptchem.tuner import Tuner\n",
    "from gptchem.formatter import RegressionFormatter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pycm import ConfusionMatrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING = {\n",
    "    \"t5\": [\"q\", \"v\"],\n",
    "    \"mt5\": [\"q\", \"v\"],\n",
    "    \"bart\": [\"q_proj\", \"v_proj\"],\n",
    "    \"gpt2\": [\"c_attn\"],\n",
    "    \"bloom\": [\"query_key_value\"],\n",
    "    \"blip-2\": [\"q\", \"v\", \"q_proj\", \"v_proj\"],\n",
    "    \"opt\": [\"q_proj\", \"v_proj\"],\n",
    "    \"gptj\": [\"q_proj\", \"v_proj\"],\n",
    "    \"gpt_neox\": [\"query_key_value\"],\n",
    "    \"gpt_neo\": [\"q_proj\", \"v_proj\"],\n",
    "    \"bert\": [\"query\", \"value\"],\n",
    "    \"roberta\": [\"query\", \"value\"],\n",
    "    \"xlm-roberta\": [\"query\", \"value\"],\n",
    "    \"electra\": [\"query\", \"value\"],\n",
    "    \"deberta-v2\": [\"query_proj\", \"value_proj\"],\n",
    "    \"deberta\": [\"in_proj\"],\n",
    "    \"layoutlm\": [\"query\", \"value\"],\n",
    "    \"llama\": [\"q_proj\", \"v_proj\"],\n",
    "    \"chatglm\": [\"query_key_value\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from typing import List\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, EarlyStoppingCallback\n",
    "import os\n",
    "print(torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the data\n",
    "def get_data():\n",
    "    with open(\"qm9_key_smiles_0_val_u0_atom_b3lyp.pickle\", 'rb') as pickle_file:\n",
    "        pickled_data=pickle.load(pickle_file)\n",
    "    raw_data = pd.DataFrame(list(pickled_data.items()), columns=[\"SMILES\", \"B3LYP atomization energy in kcal/mol\"])\n",
    "    formatter = RegressionFormatter(representation_column='SMILES',\n",
    "        label_column='B3LYP atomization energy in kcal/mol',\n",
    "        property_name='atomization energy in kcal/mol',\n",
    "        num_digits=4\n",
    "        )\n",
    "    data = formatter.format_many(raw_data).drop(columns=[\"label\",\"representation\"], axis=1)\n",
    "    train_size=round(0.8*len(data))\n",
    "    val_size=round((len(data)-train_size)/2)\n",
    "    test_size=len(data)-train_size-val_size\n",
    "    df_trainval, df_test = train_test_split(data, test_size=val_size, train_size=train_size, random_state=42)\n",
    "    df_train, df_val = train_test_split(df_trainval,test_size=test_size, shuffle=True)\n",
    "\n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    #dataframes\n",
    "    df_train: pd.DataFrame,\n",
    "    df_val: pd.DataFrame,\n",
    "    # model/data params\n",
    "    base_model: str = \"gpt2\",  # the only required argument\n",
    "    data_path: str = \"qm9_key_smiles_0_val_u0_atom_b3lyp.pickle\",\n",
    "    output_dir: str = \"outputs\",\n",
    "    # training hyperparams\n",
    "    batch_size: int = 1024,\n",
    "    micro_batch_size: int = 64,\n",
    "    num_epochs: int = 10,\n",
    "    learning_rate: float = 3e-4,\n",
    "    cutoff_len: int = 256,\n",
    "    # lora hyperparams\n",
    "    lora_r: int = 8,\n",
    "    lora_alpha: int = 16,\n",
    "    lora_dropout: float = 0.05,\n",
    "    lora_target_modules: List[str] = [\"\"],\n",
    "    # llm hyperparams\n",
    "    train_on_inputs: bool = True,  # if False, masks out inputs in loss\n",
    "    group_by_length: bool = False,  # faster, but produces an odd training loss curve\n",
    "    resume_from_checkpoint: str = None,  # either training checkpoint or final adapter\n",
    "    prompt_template_name: str = \"chemgpt\",  # The prompt template to use, will default to alpaca.\n",
    "):\n",
    "    \n",
    "    lora_target_modules = TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING[base_model]\n",
    "    if int(os.environ.get(\"LOCAL_RANK\", 0)) == 0:\n",
    "        print(\n",
    "            f\"Training LoRA model with params:\\n\"\n",
    "            f\"base_model: {base_model}\\n\"\n",
    "            f\"data_path: {data_path}\\n\"\n",
    "            f\"output_dir: {output_dir}_{base_model}\\n\"\n",
    "            f\"batch_size: {batch_size}\\n\"\n",
    "            f\"micro_batch_size: {micro_batch_size}\\n\"\n",
    "            f\"num_epochs: {num_epochs}\\n\"\n",
    "            f\"learning_rate: {learning_rate}\\n\"\n",
    "            f\"cutoff_len: {cutoff_len}\\n\"\n",
    "            f\"lora_r: {lora_r}\\n\"\n",
    "            f\"lora_alpha: {lora_alpha}\\n\"\n",
    "            f\"lora_dropout: {lora_dropout}\\n\"\n",
    "            f\"lora_target_modules: {lora_target_modules}\\n\"\n",
    "            f\"train_on_inputs: {train_on_inputs}\\n\"\n",
    "            f\"group_by_length: {group_by_length}\\n\"\n",
    "            f\"resume_from_checkpoint: {resume_from_checkpoint or False}\\n\"\n",
    "            f\"prompt template: {prompt_template_name}\\n\"\n",
    "        )\n",
    "    assert (\n",
    "        base_model\n",
    "    ), \"Please specify a --base_model, e.g. --base_model='gpt2'\"\n",
    "    gradient_accumulation_steps = batch_size // micro_batch_size\n",
    "\n",
    "    device_map = \"sequential\"\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "    #ddp = world_size != 1\n",
    "    ddp = False\n",
    "    if ddp:\n",
    "        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
    "        gradient_accumulation_steps = gradient_accumulation_steps // world_size\n",
    "    \n",
    "    #set up the model and tokenizer    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "    # might not be optimal, just trying to run the code\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model, \n",
    "        load_in_8bit=False,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map='sequential',\n",
    "    )    \n",
    "    def tokenize(prompt):\n",
    "        result = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=cutoff_len,\n",
    "            padding=True,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        return result\n",
    "    \n",
    "    def tokenize_prompt(data_point):\n",
    "        full_prompt = data_point[\"prompt\"]+data_point[\"completion\"]\n",
    "        tokenized_full_prompt = tokenize(full_prompt)\n",
    "        return tokenized_full_prompt\n",
    "\n",
    "    model = prepare_model_for_int8_training(model)\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=lora_target_modules,\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, config)\n",
    "\n",
    "    train_data = Dataset.from_pandas(df_train).shuffle().map(tokenize_prompt)\n",
    "    val_data = Dataset.from_pandas(df_val).shuffle().map(tokenize_prompt)\n",
    "    test_data = Dataset.from_pandas(df_test).shuffle().map(tokenize_prompt)\n",
    "    \n",
    "    if resume_from_checkpoint:\n",
    "        # Check the available weights and load them\n",
    "        checkpoint_name = os.path.join(\n",
    "            resume_from_checkpoint, \"pytorch_model.bin\"\n",
    "        )  # Full checkpoint\n",
    "        if not os.path.exists(checkpoint_name):\n",
    "            checkpoint_name = os.path.join(\n",
    "                resume_from_checkpoint, \"adapter_model.bin\"\n",
    "            )  # only LoRA model - LoRA config above has to fit\n",
    "            resume_from_checkpoint = (\n",
    "                False  # So the trainer won't try loading its state\n",
    "            )\n",
    "        # The two files above have a different name depending on how they were saved, but are actually the same.\n",
    "        if os.path.exists(checkpoint_name):\n",
    "            print(f\"Restarting from {checkpoint_name}\")\n",
    "            adapters_weights = torch.load(checkpoint_name)\n",
    "            model = set_peft_model_state_dict(model, adapters_weights)\n",
    "        else:\n",
    "            print(f\"Checkpoint {checkpoint_name} not found\")\n",
    "\n",
    "    model.print_trainable_parameters()  # Be more transparent about the % of trainable params.\n",
    "    \n",
    "    if not ddp and torch.cuda.device_count() > 1:\n",
    "        # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n",
    "        model.is_parallelizable = True\n",
    "        model.model_parallel = True\n",
    "   \n",
    "    trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        args=transformers.TrainingArguments(\n",
    "            per_device_train_batch_size=micro_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            warmup_steps=10,\n",
    "            num_train_epochs=num_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            fp16=True,\n",
    "            logging_steps=4,\n",
    "            optim=\"adamw_torch\",\n",
    "            evaluation_strategy=\"steps\",\n",
    "            save_strategy=\"steps\",\n",
    "            eval_steps=4,\n",
    "            save_steps=4,\n",
    "            output_dir=output_dir+\"_\"+base_model,\n",
    "            save_total_limit=3,\n",
    "            metric_for_best_model = 'eval_loss',\n",
    "            load_best_model_at_end=True,\n",
    "            ddp_find_unused_parameters=False if ddp else None,\n",
    "            group_by_length=group_by_length,\n",
    "        ),\n",
    "        callbacks = [EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "        data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "    )\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    old_state_dict = model.state_dict\n",
    "    print(old_state_dict)\n",
    "    print(model.state_dict)\n",
    "    model.state_dict = (\n",
    "        lambda self, *_, **__: get_peft_model_state_dict(\n",
    "            self, old_state_dict()\n",
    "        )\n",
    "    ).__get__(model, type(model))\n",
    "    \n",
    "    \n",
    "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "\n",
    "    model.save_pretrained(output_dir+\"_\"+base_model)\n",
    "\n",
    "    print(\n",
    "        \"\\n If there's a warning about missing keys above, please disregard :)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import GenerationConfig\n",
    "#from utils.callbacks import Iteratorize, Stream\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\" \n",
    "    \n",
    "def generate(\n",
    "    df_test: pd.DataFrame,\n",
    "    load_8bit: bool = False,\n",
    "    base_model: str = \"gpt2\",\n",
    "    lora_weights: str = \"outputs\",\n",
    "    prompt_template: str = \"\",\n",
    "    cutoff_len: int = 256,\n",
    "):\n",
    "    #set up the model and tokenizer    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "    # might not be optimal, just trying to run the code\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    def tokenize(prompt):\n",
    "        result = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=cutoff_len,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    model= AutoModelForCausalLM.from_pretrained(\n",
    "        base_model, \n",
    "        load_in_8bit=False,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map='sequential',\n",
    "    )    \n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        lora_weights+\"_\"+base_model,\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    if not load_8bit:\n",
    "        model.half()  # seems to fix bugs for some users.\n",
    "\n",
    "    model.eval()\n",
    "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    def evaluate(\n",
    "        prompt,\n",
    "        temperature=0,\n",
    "        top_p=0.75,\n",
    "        top_k=40,\n",
    "        num_beams=4,\n",
    "        max_new_tokens=128,\n",
    "        stream_output=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        inputs = tokenize(prompt)\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        generation_config = GenerationConfig(\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            num_beams=num_beams,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        generate_params = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"generation_config\": generation_config,\n",
    "            \"return_dict_in_generate\": True,\n",
    "            \"output_scores\": True,\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "        }\n",
    "\n",
    "        # Without streaming\n",
    "        with torch.no_grad():\n",
    "            generation_output = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                generation_config=generation_config,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "            )\n",
    "        s = generation_output.sequences[0]\n",
    "        output = tokenizer.decode(s)\n",
    "        #print(output)\n",
    "        return output\n",
    "\n",
    "    df_test[\"model_out\"] = df_test[\"prompt\"].map(lambda x: evaluate(x))\n",
    "    df_test[\"energy_out\"] = df_test[\"model_out\"].map(lambda x: float(x.replace('###','@@@').split('@@@')[1]))\n",
    "    df_test[\"energy_true\"] = df_test[\"completion\"].map(lambda x: float(x.split('@@@')[0]))\n",
    "    return(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2348388/3385680987.py:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"model_out\"] = df_test[\"prompt\"].map(lambda x: evaluate(x))\n",
      "/tmp/ipykernel_2348388/3385680987.py:100: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"energy_out\"] = df_test[\"model_out\"].map(lambda x: float(x.replace('###','@@@').split('@@@')[1]))\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df_train, df_val, df_test = get_data()\n",
    "    train(df_train, df_val)\n",
    "    outs=generate(df_test.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "      <th>model_out</th>\n",
       "      <th>energy_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74606</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-2071.9989@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56992</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1890.364@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16367</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1361.6393@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1653.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12238</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1466.7489@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42893</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1591.6076@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13203</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1405.7739@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82846</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1841.7596@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25217</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1599.1101@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47472</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1743.1485@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116647</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1798.0185@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104737</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1869.2652@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1653.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12947</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1483.211@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12181</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1784.618@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121548</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1607.9761@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65617</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1759.5296@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13238</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-2191.8958@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18843</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1741.6192@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1753.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41836</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1492.522@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118167</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1979.7081@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22273</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1627.3498@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46552</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-2055.0035@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122615</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1431.7735@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96842</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-2068.9536@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42403</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1943.0791@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98367</th>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1913.2166@@@</td>\n",
       "      <td>What is the atomization energy in kcal/mol of ...</td>\n",
       "      <td>-1853.906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   prompt      completion  \\\n",
       "74606   What is the atomization energy in kcal/mol of ...   -2071.9989@@@   \n",
       "56992   What is the atomization energy in kcal/mol of ...    -1890.364@@@   \n",
       "16367   What is the atomization energy in kcal/mol of ...   -1361.6393@@@   \n",
       "12238   What is the atomization energy in kcal/mol of ...   -1466.7489@@@   \n",
       "42893   What is the atomization energy in kcal/mol of ...   -1591.6076@@@   \n",
       "13203   What is the atomization energy in kcal/mol of ...   -1405.7739@@@   \n",
       "82846   What is the atomization energy in kcal/mol of ...   -1841.7596@@@   \n",
       "25217   What is the atomization energy in kcal/mol of ...   -1599.1101@@@   \n",
       "47472   What is the atomization energy in kcal/mol of ...   -1743.1485@@@   \n",
       "116647  What is the atomization energy in kcal/mol of ...   -1798.0185@@@   \n",
       "104737  What is the atomization energy in kcal/mol of ...   -1869.2652@@@   \n",
       "12947   What is the atomization energy in kcal/mol of ...    -1483.211@@@   \n",
       "12181   What is the atomization energy in kcal/mol of ...    -1784.618@@@   \n",
       "121548  What is the atomization energy in kcal/mol of ...   -1607.9761@@@   \n",
       "65617   What is the atomization energy in kcal/mol of ...   -1759.5296@@@   \n",
       "13238   What is the atomization energy in kcal/mol of ...   -2191.8958@@@   \n",
       "18843   What is the atomization energy in kcal/mol of ...   -1741.6192@@@   \n",
       "41836   What is the atomization energy in kcal/mol of ...    -1492.522@@@   \n",
       "118167  What is the atomization energy in kcal/mol of ...   -1979.7081@@@   \n",
       "22273   What is the atomization energy in kcal/mol of ...   -1627.3498@@@   \n",
       "46552   What is the atomization energy in kcal/mol of ...   -2055.0035@@@   \n",
       "122615  What is the atomization energy in kcal/mol of ...   -1431.7735@@@   \n",
       "96842   What is the atomization energy in kcal/mol of ...   -2068.9536@@@   \n",
       "42403   What is the atomization energy in kcal/mol of ...   -1943.0791@@@   \n",
       "98367   What is the atomization energy in kcal/mol of ...   -1913.2166@@@   \n",
       "\n",
       "                                                model_out  energy_out  \n",
       "74606   What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "56992   What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "16367   What is the atomization energy in kcal/mol of ...   -1653.906  \n",
       "12238   What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "42893   What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "13203   What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "82846   What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "25217   What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "47472   What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "116647  What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "104737  What is the atomization energy in kcal/mol of ...   -1653.906  \n",
       "12947   What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "12181   What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "121548  What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "65617   What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "13238   What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "18843   What is the atomization energy in kcal/mol of ...   -1753.906  \n",
       "41836   What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "118167  What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "22273   What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "46552   What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "122615  What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "96842   What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "42403   What is the atomization energy in kcal/mol of ...   -1853.906  \n",
       "98367   What is the atomization energy in kcal/mol of ...   -1853.906  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'energy_true'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/chemGPT/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/envs/chemGPT/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/chemGPT/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'energy_true'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(\u001b[43mouts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43menergy_true\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, outs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menergy_out\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/chemGPT/lib/python3.10/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/envs/chemGPT/lib/python3.10/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'energy_true'"
     ]
    }
   ],
   "source": [
    "plt.scatter(outs['energy_true'], outs['energy_out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(outs['energy_true'], outs['energy_out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemGPT",
   "language": "python",
   "name": "chemgpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
